{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN on CartPole-v1 Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:53.395282Z",
     "start_time": "2019-02-27T13:47:53.315141Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Code copi√© de https://github.com/tqjxlm/Simple-DQN-Pytorch\"\"\"\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from copy import copy\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T07:07:47.989411Z",
     "start_time": "2019-02-28T07:07:47.979866Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'Dueling_DDQN_Prior_Memory'\n",
    "save_name = 'checkpoints/' + model_name\n",
    "resume = False\n",
    "\n",
    "class Config():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 10\n",
    "        self.TARGET_UPDATE = 200\n",
    "        self.BATCH_SIZE = 256\n",
    "        self.start_from = 512\n",
    "        self.GAMMA = 1\n",
    "        self.dueling = True\n",
    "        self.plot_every = 5\n",
    "        self.lr = 3e-5\n",
    "        self.optim_method = optim.Adam\n",
    "        self.memory_size = 10000\n",
    "        self.conv_layer_settings = [\n",
    "            (3, 8, 5, 2),\n",
    "            (8, 16, 5, 2),\n",
    "            (16, 32, 5, 2),\n",
    "            (32, 32, 5, 2)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:54.664318Z",
     "start_time": "2019-02-27T13:47:53.399720Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ['state', 'action', 'reward', 'next_state', 'terminal'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=10000):\n",
    "        self.prob_alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.frame = 1\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    def push(self, transition):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0**self.prob_alpha\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        if total < self.capacity:\n",
    "            pos = total\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            prios = self.priorities[:total]\n",
    "            probs = (1 - prios / prios.sum()) / (total - 1)\n",
    "            pos = np.random.choice(total, 1, p=probs)\n",
    "\n",
    "        self.priorities[pos] = max_prio\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        total = len(self.buffer)\n",
    "        prios = self.priorities[:total]\n",
    "        probs = prios / prios.sum()\n",
    "\n",
    "        indices = np.random.choice(total, batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "\n",
    "        # Min of ALL probs, not just sampled probs\n",
    "        prob_min = probs.min()\n",
    "        max_weight = (prob_min*total)**(-beta)\n",
    "\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= max_weight\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float)\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = (prio + 1e-5)**self.prob_alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:54.678740Z",
     "start_time": "2019-02-27T13:47:54.665463Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params(net):\n",
    "\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_size, output_size, kernel_size=kernel_size, stride=stride, padding=self.padding)\n",
    "        self.bn = nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "    def size_out(self, size):\n",
    "        return (size - self.kernel_size + self.padding * 2) // self.stride + 1\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, conv_layer_settings, dueling=False):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dueling = dueling\n",
    "\n",
    "        conv_blocks = []\n",
    "        size = np.array([h, w])\n",
    "        for s in conv_layer_settings:\n",
    "            block = ConvBlock(s[0], s[1], s[2], s[3])\n",
    "            conv_blocks.append(block)\n",
    "            size = block.size_out(size)\n",
    "        self.conv_step = nn.Sequential(*conv_blocks)\n",
    "        linear_input_size = size[0] * size[1] * conv_layer_settings[-1][1]\n",
    "\n",
    "        if self.dueling:\n",
    "            self.adv = nn.Linear(linear_input_size, 2)\n",
    "            self.val = nn.Linear(linear_input_size, 1)\n",
    "        else:\n",
    "            self.head = nn.Linear(linear_input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_step(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.dueling:\n",
    "            adv = F.relu(self.adv(x))\n",
    "            val = F.relu(self.val(x))\n",
    "            return val + adv - val.mean()\n",
    "        else:\n",
    "            return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:56.896504Z",
     "start_time": "2019-02-27T13:47:54.680191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from threading import Event, Thread\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.CenterCrop((250, 500)),\n",
    "                    T.Resize(64),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "class RenderThread(Thread):\n",
    "    # Usage:\n",
    "    # 0. call env.step() or env.reset() to update env state\n",
    "    # 1. call begin_render() to schedule a rendering task (non-blocking)\n",
    "    # 2. call get_screen() to get the lastest scheduled result (block main thread if rendering not done)\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(RenderThread, self).__init__(target=self.render)\n",
    "        self._stop_event = Event()\n",
    "        self._state_event = Event()\n",
    "        self._render_event = Event()\n",
    "        self.env = env\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "        self._state_event.set()\n",
    "\n",
    "    def stopped(self):\n",
    "        return self._stop_event.is_set()\n",
    "\n",
    "    def begin_render(self):\n",
    "        self._state_event.set()\n",
    "\n",
    "    def get_screen(self):\n",
    "        self._render_event.wait()\n",
    "        self._render_event.clear()\n",
    "        return self.screen\n",
    "\n",
    "    def render(self):\n",
    "        while not self.stopped():\n",
    "            self._state_event.wait()\n",
    "            self._state_event.clear()\n",
    "\n",
    "            self.screen = self.env.render(\n",
    "                mode='rgb_array').transpose((2, 0, 1))\n",
    "            self.screen = np.ascontiguousarray(\n",
    "                self.screen, dtype=np.float32) / 255\n",
    "            self.screen = torch.from_numpy(self.screen)\n",
    "            self.screen = resize(self.screen).unsqueeze(0).to(device)\n",
    "            self._render_event.set()\n",
    "\n",
    "\n",
    "# A simple test\n",
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "env.reset()\n",
    "renderer.begin_render()\n",
    "screen = renderer.get_screen()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.cpu().squeeze(0).permute(\n",
    "    1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "renderer.stop()\n",
    "renderer.join()\n",
    "\n",
    "_, _, screen_height, screen_width = screen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:56.912375Z",
     "start_time": "2019-02-27T13:47:56.898118Z"
    }
   },
   "outputs": [],
   "source": [
    "class History():\n",
    "\n",
    "    def __init__(self, plot_size=300, plot_every=5):\n",
    "        self.plot_size = plot_size\n",
    "        self.episode_durations = deque([], self.plot_size)\n",
    "        self.means = deque([], self.plot_size)\n",
    "        self.episode_loss = deque([], self.plot_size)\n",
    "        self.indexes = deque([], self.plot_size)\n",
    "        self.step_loss = []\n",
    "        self.step_eps = []\n",
    "        self.peak_reward = 0\n",
    "        self.peak_mean = 0\n",
    "        self.moving_avg = 0\n",
    "        self.step_count = 0\n",
    "        self.total_episode = 0\n",
    "        self.plot_every = plot_every\n",
    "\n",
    "    def update(self, t, episode_loss):\n",
    "        self.episode_durations.append(t + 1)\n",
    "        self.episode_loss.append(episode_loss / (t + 1))\n",
    "        self.indexes.append(self.total_episode)\n",
    "        if t + 1 > self.peak_reward:\n",
    "            self.peak_reward = t + 1\n",
    "        if len(self.episode_durations) >= 100:\n",
    "            self.means.append(sum(list(self.episode_durations)[-100:]) / 100)\n",
    "        else:\n",
    "            self.moving_avg = self.moving_avg + \\\n",
    "                (t - self.moving_avg) / (self.total_episode + 1)\n",
    "            self.means.append(self.moving_avg)\n",
    "        if self.means[-1] > self.peak_mean:\n",
    "            self.peak_mean = self.means[-1]\n",
    "\n",
    "        if self.total_episode % self.plot_every == 0:\n",
    "            self.plot()\n",
    "\n",
    "    def plot(self):\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        f, (ax1, ax3) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        ax1.plot(self.indexes, self.episode_durations)\n",
    "        ax1.plot(self.indexes, self.means)\n",
    "        ax1.axhline(self.peak_reward, color='g')\n",
    "        ax1.axhline(self.peak_mean, color='g')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(self.indexes, self.episode_loss, 'r')\n",
    "\n",
    "        ax4 = ax3.twinx()\n",
    "        total_step = len(self.step_loss)\n",
    "        sample_rate = total_step // self.plot_size if total_step > (\n",
    "            self.plot_size * 10) else 1\n",
    "        ax3.set_title('total: {0}'.format(total_step))\n",
    "        ax3.plot(self.step_eps[::sample_rate], 'g')\n",
    "        ax4.plot(self.step_loss[::sample_rate], 'b')\n",
    "\n",
    "        plt.pause(0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Hyperparameters\n",
    "Run this cell if you want to start a fresh new training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:57.048244Z",
     "start_time": "2019-02-27T13:47:56.990560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fresh start...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv_step): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (conv): Conv2d(3, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (adv): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (val): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment this out if a checkpoint is available\n",
    "# load_name = 'checkpoints/checkpoint'\n",
    "\n",
    "# Init network\n",
    "if resume:\n",
    "    print('loading checkpoint...')\n",
    "    with open(save_name + '.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        history = data['history']\n",
    "        config = data['config']\n",
    "\n",
    "    checkpoint = torch.load(save_name + '.pt')\n",
    "\n",
    "    policy_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings).to(device)\n",
    "    target_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings).to(device)\n",
    "    policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "    target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "    optimizer = config.optim_method(policy_net.parameters(), lr=config.lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "else:\n",
    "    print('fresh start...')\n",
    "    history = History()\n",
    "    config = Config()\n",
    "\n",
    "    policy_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings, config.dueling).to(device)\n",
    "    target_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings, config.dueling).to(device)\n",
    "    init_params(policy_net)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = config.optim_method(policy_net.parameters(), lr=config.lr)\n",
    "\n",
    "memory = ReplayMemory(config.memory_size)\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:57.087923Z",
     "start_time": "2019-02-27T13:47:57.050377Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model(step):\n",
    "    if len(memory) < config.start_from:\n",
    "        return 0\n",
    "\n",
    "    if step % config.TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Sample memory as a batch\n",
    "    samples, ids, weights = memory.sample(config.BATCH_SIZE)\n",
    "    batch = Transition(*zip(*samples))\n",
    "\n",
    "    # A tensor cannot be None, so strip out terminal states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Bellman's Equation\n",
    "    with torch.no_grad():\n",
    "        online_Q = policy_net(non_final_next_states)\n",
    "        target_Q = target_net(non_final_next_states)\n",
    "        next_Q = torch.zeros(config.BATCH_SIZE, device=device)\n",
    "        next_Q[non_final_mask] = target_Q.gather(\n",
    "            1, online_Q.max(1)[1].detach().unsqueeze(1)).squeeze(1)\n",
    "        target_Q = next_Q * config.GAMMA + reward_batch\n",
    "\n",
    "    # Compute loss\n",
    "    policy_net.train()\n",
    "    current_Q = policy_net(state_batch).gather(1, action_batch)\n",
    "    diff = current_Q.squeeze() - target_Q\n",
    "    loss = (0.5 * (diff * diff) * weights).mean()\n",
    "\n",
    "    # Update memory\n",
    "    delta = diff.abs().detach().cpu().numpy().tolist()\n",
    "    memory.update_priorities(ids, delta)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return config.epsilon_final + \\\n",
    "        (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "\n",
    "def select_action(state, eps):\n",
    "    sample = random.random()\n",
    "    if sample > eps:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop\n",
    "This cell can be run multiple times without rerun the above cells.\n",
    "\n",
    "Just change the loop size and you can continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:52:40.822542Z",
     "start_time": "2019-02-27T13:47:57.088968Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Robert\\anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Robert\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-aaed048d4c04>\", line 53, in render\n",
      "    mode='rgb_array').transpose((2, 0, 1))\n",
      "  File \"C:\\Users\\Robert\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\", line 213, in render\n",
      "    return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
      "  File \"C:\\Users\\Robert\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 104, in render\n",
      "    glClearColor(1,1,1,1)\n",
      "  File \"C:\\Users\\Robert\\anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\", line 107, in errcheck\n",
      "    raise GLException(msg)\n",
      "pyglet.gl.lib.GLException: b'invalid operation'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    history.total_episode += 1\n",
    "\n",
    "    env.reset()\n",
    "    renderer.begin_render()\n",
    "\n",
    "    init_screen = renderer.get_screen()\n",
    "    screens = deque([init_screen] * 3, 3)\n",
    "    state = torch.cat(list(screens), dim=1)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for t in count():\n",
    "        history.step_count += 1\n",
    "\n",
    "        # Select and perform an action\n",
    "        eps = epsilon_by_frame(history.total_episode)\n",
    "        action = select_action(state, eps)\n",
    "        history.step_eps.append(eps)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Render the step in another thread\n",
    "        renderer.begin_render()\n",
    "\n",
    "        # Do optimization in main thread\n",
    "        loss = optimize_model(history.step_count)\n",
    "        avg_loss += loss\n",
    "        history.step_loss.append(loss)\n",
    "\n",
    "        # Render the next_state and remember it\n",
    "        screens.append(renderer.get_screen())\n",
    "        next_state = torch.cat(list(screens), dim=1) if not done else None\n",
    "        memory.push(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            history.update(t, avg_loss)\n",
    "            break\n",
    "\n",
    "renderer.stop()\n",
    "renderer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T07:09:10.538262Z",
     "start_time": "2019-02-28T07:09:03.916105Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'policy_net': policy_net.state_dict(),\n",
    "    'target_net': target_net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, save_name + '.pt')\n",
    "\n",
    "with open(save_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump({'history': history, 'config': config},\n",
    "                f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T11:03:48.917258Z",
     "start_time": "2019-02-23T11:03:48.891469Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(save_name + '.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    history = data['history']\n",
    "    config = data['config']\n",
    "\n",
    "checkpoint = torch.load(save_name + '.pt')\n",
    "policy_net = DQN(screen_height, screen_width,\n",
    "                 config.conv_layer_settings, config.dueling).to(device)\n",
    "policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T10:39:04.260900Z",
     "start_time": "2019-02-23T10:38:59.345035Z"
    }
   },
   "outputs": [],
   "source": [
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    renderer.begin_render()\n",
    "\n",
    "    init_screen = renderer.get_screen()\n",
    "    screens = deque([init_screen] * 3, 3)\n",
    "    state = torch.cat(list(screens), dim=1)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, 0)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the step in another thread\n",
    "        renderer.begin_render()\n",
    "\n",
    "        # Render the next_state and remember it\n",
    "        screens.append(renderer.get_screen())\n",
    "        next_state = torch.cat(list(screens), dim=1) if not done else None\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('total reward:', total_reward)\n",
    "            break\n",
    "\n",
    "renderer.stop()\n",
    "renderer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
