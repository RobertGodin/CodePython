{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- > epoch 1:  erreur quadratique moyenne 2.437975009563033\n",
      "-------- > epoch 2:  erreur quadratique moyenne 2.383259292384644\n",
      "-------- > epoch 3:  erreur quadratique moyenne 2.3760433181069325\n",
      "-------- > epoch 4:  erreur quadratique moyenne 2.3770713108458454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d8c01a2f87f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;31m# Entrainer le RNA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m un_RNA.entrainer_descente_gradiant_en_lot(donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n\u001b[1;32m--> 432\u001b[1;33m                                                 nb_epochs=10,taux=0.3,trace = False, graph_cout = True)\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d8c01a2f87f2>\u001b[0m in \u001b[0;36mentrainer_descente_gradiant_en_lot\u001b[1;34m(self, donnees_ent_X, donnees_ent_Y, donnees_test_X, donnees_test_Y, nb_epochs, taux, trace, graph_cout)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcouche\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcouches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                     \u001b[0mXY_propage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcouche\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropager_une_couche\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXY_propage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Valeur de Y après propagation pour la couche:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXY_propage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d8c01a2f87f2>\u001b[0m in \u001b[0;36mpropager_une_couche\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "# Implémentation d'un RNA par couche\n",
    "# Deux types de couches : dense linéaire et activation\n",
    "# Division des données en deux groupes : entrainement et test\n",
    "\n",
    "# Exemple avec MNIST\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42) # pour reproduire les mêmes résultats\n",
    "import random\n",
    "random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "class Couche:\n",
    "    \"\"\" Classe abstraite qui représente une couche du RNA\n",
    "        X:  np.array 2D de taille (1,n), entrée de la couche \n",
    "        Y: np.array 2D de taille (1,m), sortie de la couche\n",
    "    \"\"\"\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Calculer la sortie Y pour une valeur de X\n",
    "        \n",
    "        X : vecteur des variables prédictives\n",
    "        Les valeurs de X et Y sont stockées pour les autres traitements.\n",
    "        \"\"\"\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées par rapport à X et les autres paramètres à partir de dJ_dY\n",
    "        et mettre à jour les paramètres de la couche selon le taux spécifié.\n",
    "        \n",
    "        dJ_dY : np.array(1,m), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "\n",
    "class CoucheDenseLineaire(Couche):\n",
    "    \"\"\" Couche linéaire dense. Y=WX+B\n",
    "    \"\"\"\n",
    "    def __init__(self,n,m,init_W=None,init_B=None):\n",
    "        \"\"\" Initilalise les paramètres de la couche. W et B sont initialisés avec init_W et init_B lorsque spécifiés.\n",
    "        Sinon, des valeurs aléatoires sont générés pour W une distribution normale et B est initialisée avec des 0 \n",
    "        si les paramètres init_W et init_B ne sont pas spécifiés.\n",
    "        L'initialization He est employée pour W\n",
    "        n : int, taille du vecteur d'entrée X\n",
    "        m : int, taille du vecteur de sortie Y\n",
    "        init_W : np.array, shape(n,m), valeur initiale optionnelle de W\n",
    "        init_B : np.array, shape(1,m), valeur initial optionnelle de B\n",
    "        \"\"\"\n",
    "        if init_W is None :\n",
    "            # Initialization He\n",
    "            self.W = np.random.randn(n,m) * np.sqrt(2/n) \n",
    "        else:\n",
    "            self.W = init_W\n",
    "        if init_B is None :\n",
    "            self.B = np.zeros((1,m))\n",
    "        else:\n",
    "            self.B = init_B\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Fait la propagation de X et retourne Y=WX+B. \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.B + np.dot(self.X,self.W)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées dJ_dW,dJ_dB,dJ_dX pour une couche dense linéaire et\n",
    "        mettre à jour les paramètres selon le taux spécifié\n",
    "        \n",
    "        dJ_dY : np.array(1,2), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "        dJ_dW = np.dot(self.X.T,dJ_dY)\n",
    "        dJ_dB = dJ_dY\n",
    "        dJ_dX = np.dot(dJ_dY,self.W.T)\n",
    "        if trace:\n",
    "            print(\"dJ_dW:\",dJ_dW)\n",
    "            print(\"dJ_dB:\",dJ_dB)\n",
    "            print(\"dJ_dX:\",dJ_dX)\n",
    "        # Metre à jour les paramètres W et B\n",
    "        self.W -= taux * dJ_dW\n",
    "        self.B -= taux * dJ_dB\n",
    "        if trace:\n",
    "            print(\"W modifié:\",self.W)\n",
    "            print(\"B modifié:\",self.B)\n",
    "        return dJ_dX\n",
    "\n",
    "    \n",
    "class CoucheActivation(Couche):\n",
    "    \"\"\" Couche d'activation selon une fonction spécifiée dans le constructeur\n",
    "    \"\"\"\n",
    "    def __init__(self,fonction_activation,derivee):\n",
    "        \"\"\" Initialise la fonction_activation ainsi que la dérivée\n",
    "        fonction_activation: une fonction qui prend chacune des valeurs de X et \n",
    "        retourne Y=fonction_activation(X)\n",
    "        derivee: une fonction qui calcule la dérivée la fonction_activation\n",
    "        \"\"\"\n",
    "        self.fonction_activation = fonction_activation\n",
    "        self.derivee = derivee\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Retourne Y=fonction_activation(X)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.fonction_activation(self.X)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Retourne la dérivée de la fonction d'activation par rapport l'entrée X\n",
    "        Le taux n'est pas utilisé parce qu'il n'y a pas de paramètres à modifier dans ce genre de couche\n",
    "        \"\"\"\n",
    "        return self.derivee(self.X) * dJ_dY\n",
    "\n",
    "class CoucheSoftmax(Couche):\n",
    "    \"\"\" Couche d'activation softmax\n",
    "    \"\"\"\n",
    "    def __init__(self,n):\n",
    "        \"\"\"\n",
    "        n: nombre d'entrées et de sorties\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Calcule les activations softmax pour chacunes de entrées xi\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        X_decale = X-np.max(X) # Pour la stabilité numérique, les valeurs sont décalées de max(X)\n",
    "        exponentielles = np.exp(X_decale)\n",
    "        self.Y = exponentielles / np.sum(exponentielles)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Retourne la dérivée de la fonction d'activation par rapport l'entrée X\n",
    "        Le taux n'est pas utilisé parce qu'il n'y a pas de paramètres à modifier dans ce genre de couche\n",
    "        \"\"\"\n",
    "        return np.dot(dJ_dY,self.Y.T*(np.identity(self.n)-self.Y))\n",
    "\n",
    "def erreur_quadratique(y_prediction,y):\n",
    "    \"\"\" Retourne l'erreur quadratique entre la prédiction y_prediction et la valeur attendue y\n",
    "    \"\"\" \n",
    "    return np.sum(np.power(y_prediction-y,2))\n",
    "\n",
    "def d_erreur_quadratique(y_prediction,y):\n",
    "    return 2*(y_prediction-y)\n",
    "\n",
    "def entropie_croisee(y_prediction,y):\n",
    "    \"\"\" Retourne l'entropie croisée entre la prédiction y_prediction et la valeur attendue y\n",
    "    \"\"\" \n",
    "    return -np.sum(y*np.log(y_prediction))\n",
    "\n",
    "def d_entropie_croisee(y_prediction,y):\n",
    "    return -(y/y_prediction)\n",
    "\n",
    "class ReseauMultiCouches:\n",
    "    \"\"\" Réseau mutli-couche formé par une séquence de Couches\n",
    "    \n",
    "    couches : liste de Couches du RNA\n",
    "    cout : fonction qui calcule de cout J\n",
    "    derivee_cout: dérivée de la fonction de cout\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.couches = []\n",
    "        self.cout = None\n",
    "        self.derivee_cout = None\n",
    "\n",
    "    def ajouter_couche(self,couche):\n",
    "        self.couches.append(couche)\n",
    "\n",
    "    def specifier_J(self,cout,derivee_cout):\n",
    "        \"\"\" Spécifier la fonction de coût J et sa dérivée\n",
    "        \"\"\"\n",
    "        self.cout = cout\n",
    "        self.derivee_cout = derivee_cout\n",
    "\n",
    "    def propagation_donnees_X(self,donnees_X,trace=False):\n",
    "        \"\"\" Prédire Y pour chacune des observations dans donnees_X)\n",
    "        donnees_X : np.array 3D des valeurs de X pour chacune des observations\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_X)\n",
    "        predictions_Y = []\n",
    "        for indice_observation in range(nb_observations):\n",
    "            # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "            # à la valeur de Y de la couche précédente\n",
    "            XY_propage = donnees_X[indice_observation]\n",
    "            if trace: \n",
    "                print(\"Valeur de X initiale:\",XY_propage)\n",
    "            for couche in self.couches:\n",
    "                XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                if trace: \n",
    "                    print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "            predictions_Y.append(XY_propage)\n",
    "\n",
    "        return predictions_Y\n",
    "\n",
    "    def metriques(self, donnees_X,donnees_Y):\n",
    "        \"\"\"Retourne le cout moyen, la proportion de bons résultats\n",
    "        Choisit l'indice de la classe dont l'activation est la plus grande\"\"\"\n",
    "        erreur_quadratique = 0\n",
    "        nb_correct = 0\n",
    "        predictions_Y=self.propagation_donnees_X(donnees_X)\n",
    "        for indice in range(len(donnees_Y)):\n",
    "            erreur_quadratique += self.cout(predictions_Y[indice],donnees_Y[indice])\n",
    "            classe_predite = np.argmax(predictions_Y[indice])\n",
    "            if donnees_Y[indice][0,classe_predite] == 1:\n",
    "                nb_correct+=1\n",
    "        return (erreur_quadratique/len(donnees_Y),nb_correct/len(donnees_Y))\n",
    "\n",
    "    def entrainer_descente_gradiant_stochastique(self,donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                 nb_epochs,taux,trace=False,graph_cout=False):\n",
    "        \"\"\" Entrainer le réseau par descente de gradiant stochastique (une observation à la fois)\n",
    "        \n",
    "        donnees_ent_X : np.array 3D des valeurs de X pour chacune des observations d'entrainement\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_ent_Y : np.array 3D des valeurs de Y pour chacune des observations d'entrainement\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        donnees_test_X : np.array 3D des valeurs de X pour chacune des observations de test\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_test_Y : np.array 3D des valeurs de Y pour chacune des observations de test\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        nb_epochs : nombre de cycle de passage sur les données d'entainement\n",
    "        taux : taux dans la descente de gradiant\n",
    "        trace : Boolean, True pour afficher une trace des calculs effectués sur les paramètres\n",
    "        graph_cout : Boolean, True pur afficher un graphique de l'évolution du coût\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_ent_X)\n",
    "        if graph_cout :\n",
    "            liste_cout_moyen_ent = []\n",
    "            liste_ok_ent = []\n",
    "            liste_cout_moyen_test = []\n",
    "            liste_ok_test = []\n",
    "\n",
    "        # Boucle d'entrainement principale, nb_epochs fois\n",
    "        for cycle in range(nb_epochs):\n",
    "            cout_total = 0\n",
    "            # Descente de gradiant stochastique, une observation à la fois\n",
    "            for indice_observation in range(nb_observations):\n",
    "                # Propagation avant pour une observation X\n",
    "                # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "                # à la valeur de Y de la couche précédente\n",
    "                XY_propage = donnees_ent_X[indice_observation]\n",
    "                if trace: \n",
    "                    print(\"Valeur de X initiale:\",XY_propage)\n",
    "\n",
    "                for couche in self.couches:\n",
    "                    XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                    if trace: \n",
    "                        print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "\n",
    "                # Calcul du coût pour une observation\n",
    "                cout_total += self.cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "\n",
    "                # Rétropropagation pour une observation\n",
    "                # dJ_dX_dJ_dY représente la valeur de la dérivée dJ_dX de la couche suivante\n",
    "                # qui correspond à dJ_dY de la couche en cours de traitement\n",
    "                dJ_dX_dJ_dY = self.derivee_cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "                if trace :\n",
    "                    print(\"dJ_dY pour la couche finale:\",dJ_dX_dJ_dY)\n",
    "                for couche in reversed(self.couches):\n",
    "                    dJ_dX_dJ_dY = couche.retropropager_une_couche(dJ_dX_dJ_dY,taux,trace)\n",
    "\n",
    "            # Calculer et afficher le coût moyen pour une epoch\n",
    "            cout_moyen = cout_total/nb_observations\n",
    "            if graph_cout:\n",
    "                print(f'-------- > epoch {cycle+1}:  coût moyen {cout_moyen}')\n",
    "                cout_ent,ok_ent = self.metriques(donnees_ent_X,donnees_ent_Y)\n",
    "                cout_test,ok_test = self.metriques(donnees_test_X,donnees_test_Y)\n",
    "                liste_cout_moyen_ent.append(cout_ent)\n",
    "                liste_ok_ent.append(ok_ent)\n",
    "                liste_cout_moyen_test.append(cout_test)\n",
    "                liste_ok_test.append(ok_test)\n",
    "                \n",
    "            \n",
    "        # Affichage du graphique d'évolution de l'erreur quadratique\n",
    "        if graph_cout:\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_ent,label='Erreur entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_test,label='Erreur test')\n",
    "            plt.title(\"Evolution du coût\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_ent,label='entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_test,label='test')\n",
    "            plt.title(\"Evolution du taux de bonnes prédictions\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "\n",
    "    def entrainer_descente_gradiant_en_lot(self,donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                 nb_epochs,taux,trace=False,graph_cout=False):\n",
    "        \"\"\" Entrainer le réseau par descente de gradiant stochastique (une observation à la fois)\n",
    "        \n",
    "        donnees_ent_X : np.array 3D des valeurs de X pour chacune des observations d'entrainement\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_ent_Y : np.array 3D des valeurs de Y pour chacune des observations d'entrainement\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        donnees_test_X : np.array 3D des valeurs de X pour chacune des observations de test\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_test_Y : np.array 3D des valeurs de Y pour chacune des observations de test\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        nb_epochs : nombre de cycle de passage sur les données d'entainement\n",
    "        taux : taux dans la descente de gradiant\n",
    "        trace : Boolean, True pour afficher une trace des calculs effectués sur les paramètres\n",
    "        graph_cout : Boolean, True pur afficher un graphique de l'évolution du coût\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_ent_X)\n",
    "        if graph_cout :\n",
    "            liste_cout_moyen_ent = []\n",
    "            liste_ok_ent = []\n",
    "            liste_cout_moyen_test = []\n",
    "            liste_ok_test = []\n",
    "\n",
    "        # Boucle d'entrainement principale, nb_epochs fois\n",
    "        for cycle in range(nb_epochs):\n",
    "            cout_total = 0\n",
    "            derivee_cout_total = 0\n",
    "            # Descente de gradiant en lot, calculer l'erreur et la dérivée pour toutes les observations\n",
    "            for indice_observation in range(nb_observations):\n",
    "                # Propagation avant pour une observation X\n",
    "                # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "                # à la valeur de Y de la couche précédente\n",
    "                XY_propage = donnees_ent_X[indice_observation]\n",
    "                if trace: \n",
    "                    print(\"Valeur de X initiale:\",XY_propage)\n",
    "\n",
    "                for couche in self.couches:\n",
    "                    XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                    if trace: \n",
    "                        print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "\n",
    "                # Calcul du coût pour une observation\n",
    "                cout_total += self.cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "                derivee_cout_total += self.derivee_cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "\n",
    "\n",
    "            # Calculer et afficher le coût moyen pour une epoch\n",
    "            cout_moyen = cout_total/nb_observations\n",
    "            derivee_cout_moyen = derivee_cout_total/nb_observations\n",
    "            \n",
    "            # Rétropropagation pour le lot\n",
    "            # dJ_dX_dJ_dY représente la valeur de la dérivée dJ_dX de la couche suivante\n",
    "            # qui correspond à dJ_dY de la couche en cours de traitement\n",
    "            dJ_dX_dJ_dY = derivee_cout_moyen\n",
    "            if trace :\n",
    "                print(\"dJ_dY pour la couche finale:\",dJ_dX_dJ_dY)\n",
    "            for couche in reversed(self.couches):\n",
    "                dJ_dX_dJ_dY = couche.retropropager_une_couche(dJ_dX_dJ_dY,taux,trace)\n",
    "            \n",
    "            if graph_cout:\n",
    "                print(f'-------- > epoch {cycle+1}:  erreur quadratique moyenne {cout_moyen}')\n",
    "                cout_ent,ok_ent = self.metriques(donnees_ent_X,donnees_ent_Y)\n",
    "                cout_test,ok_test = self.metriques(donnees_test_X,donnees_test_Y)\n",
    "                liste_cout_moyen_ent.append(cout_ent)\n",
    "                liste_ok_ent.append(ok_ent)\n",
    "                liste_cout_moyen_test.append(cout_test)\n",
    "                liste_ok_test.append(ok_test)\n",
    "                \n",
    "            \n",
    "        # Affichage du graphique d'évolution de l'erreur quadratique\n",
    "        if graph_cout:\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_ent,label='Erreur entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_test,label='Erreur test')\n",
    "            plt.title(\"Evolution du coût\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_ent,label='entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_test,label='test')\n",
    "            plt.title(\"Evolution du taux de bonnes prédictions\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()            \n",
    "            \n",
    "            \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivee_tanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoide(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def derivee_sigmoide(x):\n",
    "    return sigmoide(x)*(1-sigmoide(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def derivee_relu(x):\n",
    "    return np.heaviside(x,1)\n",
    "\n",
    "def bitmap(classe):\n",
    "    \"\"\" Representer l'entier de classe par un vecteur bitmap (10,1) \n",
    "    classe : entier (entre 0 et 9 qui représente la classe de l'observation\"\"\"\n",
    "    e = np.zeros((1,10))\n",
    "    e[0,classe] = 1.0\n",
    "    return e\n",
    "\n",
    "# Chargement des données de MNIST\n",
    "import pickle, gzip\n",
    "\n",
    "fichier_donnees = gzip.open(r\"mnist.pkl.gz\", 'rb')\n",
    "donnees_ent, donnees_validation, donnees_test = pickle.load(fichier_donnees, encoding='latin1')\n",
    "fichier_donnees.close()\n",
    "    \n",
    "donnees_ent_X = donnees_ent[0].reshape((50000,1,784))\n",
    "donnees_ent_Y = [bitmap(y) for y in donnees_ent[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "donnees_test_X = donnees_test[0].reshape((10000,1,784))\n",
    "donnees_test_Y = [bitmap(y) for y in donnees_test[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "\n",
    "# Définir l'architecture du RNA \n",
    "# Deux couches denses linéaires suivies chacune d'une couche d'activation sigmoide\n",
    "un_RNA = ReseauMultiCouches()\n",
    "un_RNA.specifier_J(entropie_croisee,d_entropie_croisee)\n",
    "un_RNA.ajouter_couche(CoucheDenseLineaire(784,30))\n",
    "un_RNA.ajouter_couche(CoucheActivation(relu,derivee_relu))\n",
    "un_RNA.ajouter_couche(CoucheDenseLineaire(30,10))\n",
    "un_RNA.ajouter_couche(CoucheSoftmax(10))\n",
    "\n",
    "# Entrainer le RNA\n",
    "un_RNA.entrainer_descente_gradiant_en_lot(donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                nb_epochs=10,taux=0.3,trace = False, graph_cout = True)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Classe de l'image\",i,\":\",donnees_ent_Y[i])\n",
    "    print(\"Prédiction softmax:\",un_RNA.propagation_donnees_X(donnees_ent_X[i]))\n",
    "    image_applatie = donnees_ent_X[i]\n",
    "    une_image = image_applatie.reshape(28, 28)\n",
    "    plt.imshow(une_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
