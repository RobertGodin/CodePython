{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement sur  cpu\n",
      "-------- > epoch 0 lot 0 :  coût = 8.288488388061523\n",
      "Temps écoulé : 0m 0s\n",
      "-------- > epoch 0 lot 100 :  coût = 6.37285852432251\n",
      "Temps écoulé : 0m 3s\n",
      "-------- > epoch 0 lot 200 :  coût = 6.4168195724487305\n",
      "Temps écoulé : 0m 6s\n",
      "-------- > epoch 0 lot 300 :  coût = 6.325267314910889\n",
      "Temps écoulé : 0m 9s\n",
      "-------- > epoch 0 lot 400 :  coût = 6.300807476043701\n",
      "Temps écoulé : 0m 12s\n",
      "-------- > epoch 0 lot 500 :  coût = 5.721706390380859\n",
      "Temps écoulé : 0m 15s\n",
      "-------- > epoch 0 lot 600 :  coût = 7.842543125152588\n",
      "Temps écoulé : 0m 17s\n",
      "-------- > epoch 0 lot 700 :  coût = 7.548484802246094\n",
      "Temps écoulé : 0m 20s\n",
      "-------- > epoch 0 lot 800 :  coût = 5.539044380187988\n",
      "Temps écoulé : 0m 23s\n",
      "-------- > epoch 0 lot 900 :  coût = 5.960934162139893\n",
      "Temps écoulé : 0m 26s\n",
      "-------- > epoch 0 lot 1000 :  coût = 4.524051189422607\n",
      "Temps écoulé : 0m 29s\n",
      "-------- > epoch 0 lot 1100 :  coût = 7.4533491134643555\n",
      "Temps écoulé : 0m 32s\n",
      "-------- > epoch 0 lot 1200 :  coût = 2.2234573364257812\n",
      "Temps écoulé : 0m 34s\n",
      "-------- > epoch 0 lot 1300 :  coût = 5.4529805183410645\n",
      "Temps écoulé : 0m 37s\n",
      "-------- > epoch 0 lot 1400 :  coût = 6.221846580505371\n",
      "Temps écoulé : 0m 40s\n",
      "-------- > epoch 0 lot 1500 :  coût = 5.4514994621276855\n",
      "Temps écoulé : 0m 43s\n",
      "-------- > epoch 0 lot 1600 :  coût = 5.254108905792236\n",
      "Temps écoulé : 0m 46s\n",
      "-------- > epoch 0 lot 1700 :  coût = 4.61732816696167\n",
      "Temps écoulé : 0m 49s\n",
      "-------- > epoch 0 lot 1800 :  coût = 4.910546779632568\n",
      "Temps écoulé : 0m 52s\n",
      "-------- > epoch 0 lot 1900 :  coût = 5.030243396759033\n",
      "Temps écoulé : 0m 54s\n",
      "-------- > epoch 0 lot 2000 :  coût = 5.632030963897705\n",
      "Temps écoulé : 0m 57s\n",
      "-------- > epoch 1 lot 0 :  coût = 3.774651288986206\n",
      "Temps écoulé : 0m 58s\n",
      "-------- > epoch 1 lot 100 :  coût = 5.303441524505615\n",
      "Temps écoulé : 1m 1s\n",
      "-------- > epoch 1 lot 200 :  coût = 3.8409183025360107\n",
      "Temps écoulé : 1m 4s\n",
      "-------- > epoch 1 lot 300 :  coût = 5.160902976989746\n",
      "Temps écoulé : 1m 7s\n",
      "-------- > epoch 1 lot 400 :  coût = 4.704206943511963\n",
      "Temps écoulé : 1m 10s\n",
      "-------- > epoch 1 lot 500 :  coût = 4.933005332946777\n",
      "Temps écoulé : 1m 13s\n",
      "-------- > epoch 1 lot 600 :  coût = 6.267233371734619\n",
      "Temps écoulé : 1m 16s\n",
      "-------- > epoch 1 lot 700 :  coût = 6.398311614990234\n",
      "Temps écoulé : 1m 19s\n",
      "-------- > epoch 1 lot 800 :  coût = 4.372693061828613\n",
      "Temps écoulé : 1m 22s\n",
      "-------- > epoch 1 lot 900 :  coût = 4.809317588806152\n",
      "Temps écoulé : 1m 25s\n",
      "-------- > epoch 1 lot 1000 :  coût = 3.7362051010131836\n",
      "Temps écoulé : 1m 28s\n",
      "-------- > epoch 1 lot 1100 :  coût = 6.315938472747803\n",
      "Temps écoulé : 1m 31s\n",
      "-------- > epoch 1 lot 1200 :  coût = 0.719201385974884\n",
      "Temps écoulé : 1m 34s\n",
      "-------- > epoch 1 lot 1300 :  coût = 4.183592796325684\n",
      "Temps écoulé : 1m 37s\n",
      "-------- > epoch 1 lot 1400 :  coût = 5.636836528778076\n",
      "Temps écoulé : 1m 40s\n",
      "-------- > epoch 1 lot 1500 :  coût = 4.405368328094482\n",
      "Temps écoulé : 1m 43s\n",
      "-------- > epoch 1 lot 1600 :  coût = 3.125997304916382\n",
      "Temps écoulé : 1m 46s\n",
      "-------- > epoch 1 lot 1700 :  coût = 3.507263422012329\n",
      "Temps écoulé : 1m 49s\n",
      "-------- > epoch 1 lot 1800 :  coût = 3.6595711708068848\n",
      "Temps écoulé : 1m 52s\n",
      "-------- > epoch 1 lot 1900 :  coût = 4.049155235290527\n",
      "Temps écoulé : 1m 55s\n",
      "-------- > epoch 1 lot 2000 :  coût = 4.489012718200684\n",
      "Temps écoulé : 1m 58s\n",
      "-------- > epoch 2 lot 0 :  coût = 3.302553415298462\n",
      "Temps écoulé : 1m 59s\n",
      "-------- > epoch 2 lot 100 :  coût = 5.1691179275512695\n",
      "Temps écoulé : 2m 2s\n",
      "-------- > epoch 2 lot 200 :  coût = 2.9556989669799805\n",
      "Temps écoulé : 2m 5s\n",
      "-------- > epoch 2 lot 300 :  coût = 4.545626640319824\n",
      "Temps écoulé : 2m 8s\n",
      "-------- > epoch 2 lot 400 :  coût = 4.0055928230285645\n",
      "Temps écoulé : 2m 11s\n",
      "-------- > epoch 2 lot 500 :  coût = 4.436849594116211\n",
      "Temps écoulé : 2m 15s\n",
      "-------- > epoch 2 lot 600 :  coût = 5.044704914093018\n",
      "Temps écoulé : 2m 18s\n",
      "-------- > epoch 2 lot 700 :  coût = 5.433008670806885\n",
      "Temps écoulé : 2m 21s\n",
      "-------- > epoch 2 lot 800 :  coût = 3.7455124855041504\n",
      "Temps écoulé : 2m 24s\n",
      "-------- > epoch 2 lot 900 :  coût = 4.0677876472473145\n",
      "Temps écoulé : 2m 26s\n",
      "-------- > epoch 2 lot 1000 :  coût = 3.321958065032959\n",
      "Temps écoulé : 2m 29s\n",
      "-------- > epoch 2 lot 1100 :  coût = 5.407035827636719\n",
      "Temps écoulé : 2m 32s\n",
      "-------- > epoch 2 lot 1200 :  coût = 0.5413365364074707\n",
      "Temps écoulé : 2m 35s\n",
      "-------- > epoch 2 lot 1300 :  coût = 3.3264353275299072\n",
      "Temps écoulé : 2m 38s\n",
      "-------- > epoch 2 lot 1400 :  coût = 5.252309799194336\n",
      "Temps écoulé : 2m 41s\n",
      "-------- > epoch 2 lot 1500 :  coût = 3.684196710586548\n",
      "Temps écoulé : 2m 44s\n",
      "-------- > epoch 2 lot 1600 :  coût = 2.0816280841827393\n",
      "Temps écoulé : 2m 47s\n",
      "-------- > epoch 2 lot 1700 :  coût = 3.0107743740081787\n",
      "Temps écoulé : 2m 50s\n",
      "-------- > epoch 2 lot 1800 :  coût = 3.124518394470215\n",
      "Temps écoulé : 2m 53s\n",
      "-------- > epoch 2 lot 1900 :  coût = 3.2959563732147217\n",
      "Temps écoulé : 2m 56s\n",
      "-------- > epoch 2 lot 2000 :  coût = 3.7060024738311768\n",
      "Temps écoulé : 2m 59s\n",
      "-------- > epoch 3 lot 0 :  coût = 2.914449691772461\n",
      "Temps écoulé : 3m 0s\n",
      "-------- > epoch 3 lot 100 :  coût = 5.036782264709473\n",
      "Temps écoulé : 3m 3s\n",
      "-------- > epoch 3 lot 200 :  coût = 2.4213101863861084\n",
      "Temps écoulé : 3m 6s\n",
      "-------- > epoch 3 lot 300 :  coût = 3.9626376628875732\n",
      "Temps écoulé : 3m 9s\n",
      "-------- > epoch 3 lot 400 :  coût = 3.522273063659668\n",
      "Temps écoulé : 3m 12s\n",
      "-------- > epoch 3 lot 500 :  coût = 3.9922127723693848\n",
      "Temps écoulé : 3m 15s\n",
      "-------- > epoch 3 lot 600 :  coût = 4.18898868560791\n",
      "Temps écoulé : 3m 18s\n",
      "-------- > epoch 3 lot 700 :  coût = 4.4653849601745605\n",
      "Temps écoulé : 3m 21s\n",
      "-------- > epoch 3 lot 800 :  coût = 3.338712692260742\n",
      "Temps écoulé : 3m 24s\n",
      "-------- > epoch 3 lot 900 :  coût = 3.479856014251709\n",
      "Temps écoulé : 3m 27s\n",
      "-------- > epoch 3 lot 1000 :  coût = 3.003537893295288\n",
      "Temps écoulé : 3m 30s\n",
      "-------- > epoch 3 lot 1100 :  coût = 4.5104193687438965\n",
      "Temps écoulé : 3m 33s\n",
      "-------- > epoch 3 lot 1200 :  coût = 0.49057623744010925\n",
      "Temps écoulé : 3m 36s\n",
      "-------- > epoch 3 lot 1300 :  coût = 2.859245538711548\n",
      "Temps écoulé : 3m 39s\n",
      "-------- > epoch 3 lot 1400 :  coût = 5.042575359344482\n",
      "Temps écoulé : 3m 42s\n",
      "-------- > epoch 3 lot 1500 :  coût = 3.1459381580352783\n",
      "Temps écoulé : 3m 45s\n",
      "-------- > epoch 3 lot 1600 :  coût = 1.7714000940322876\n",
      "Temps écoulé : 3m 47s\n",
      "-------- > epoch 3 lot 1700 :  coût = 2.707648754119873\n",
      "Temps écoulé : 3m 50s\n",
      "-------- > epoch 3 lot 1800 :  coût = 2.8000268936157227\n",
      "Temps écoulé : 3m 53s\n",
      "-------- > epoch 3 lot 1900 :  coût = 2.762718439102173\n",
      "Temps écoulé : 3m 56s\n",
      "-------- > epoch 3 lot 2000 :  coût = 3.0914669036865234\n",
      "Temps écoulé : 3m 59s\n",
      "-------- > epoch 4 lot 0 :  coût = 2.6744987964630127\n",
      "Temps écoulé : 4m 0s\n",
      "-------- > epoch 4 lot 100 :  coût = 4.8652873039245605\n",
      "Temps écoulé : 4m 3s\n",
      "-------- > epoch 4 lot 200 :  coût = 2.049811840057373\n",
      "Temps écoulé : 4m 6s\n",
      "-------- > epoch 4 lot 300 :  coût = 3.485546588897705\n",
      "Temps écoulé : 4m 9s\n",
      "-------- > epoch 4 lot 400 :  coût = 3.148665428161621\n",
      "Temps écoulé : 4m 12s\n",
      "-------- > epoch 4 lot 500 :  coût = 3.7014966011047363\n",
      "Temps écoulé : 4m 15s\n",
      "-------- > epoch 4 lot 600 :  coût = 3.673776149749756\n",
      "Temps écoulé : 4m 18s\n",
      "-------- > epoch 4 lot 700 :  coût = 3.66099214553833\n",
      "Temps écoulé : 4m 21s\n",
      "-------- > epoch 4 lot 800 :  coût = 3.0431408882141113\n",
      "Temps écoulé : 4m 24s\n",
      "-------- > epoch 4 lot 900 :  coût = 3.017897129058838\n",
      "Temps écoulé : 4m 27s\n",
      "-------- > epoch 4 lot 1000 :  coût = 2.70389723777771\n",
      "Temps écoulé : 4m 30s\n",
      "-------- > epoch 4 lot 1100 :  coût = 3.7494945526123047\n",
      "Temps écoulé : 4m 33s\n",
      "-------- > epoch 4 lot 1200 :  coût = 0.44889959692955017\n",
      "Temps écoulé : 4m 36s\n",
      "-------- > epoch 4 lot 1300 :  coût = 2.5844767093658447\n",
      "Temps écoulé : 4m 38s\n",
      "-------- > epoch 4 lot 1400 :  coût = 4.902103424072266\n",
      "Temps écoulé : 4m 41s\n",
      "-------- > epoch 4 lot 1500 :  coût = 2.780022621154785\n",
      "Temps écoulé : 4m 44s\n",
      "-------- > epoch 4 lot 1600 :  coût = 1.5606839656829834\n",
      "Temps écoulé : 4m 47s\n",
      "-------- > epoch 4 lot 1700 :  coût = 2.5033023357391357\n",
      "Temps écoulé : 4m 50s\n",
      "-------- > epoch 4 lot 1800 :  coût = 2.539816379547119\n",
      "Temps écoulé : 4m 53s\n",
      "-------- > epoch 4 lot 1900 :  coût = 2.35965633392334\n",
      "Temps écoulé : 4m 56s\n",
      "-------- > epoch 4 lot 2000 :  coût = 2.564692735671997\n",
      "Temps écoulé : 4m 59s\n",
      "Texte stochastique 1 : ['tell', 'me', 'your', 'way', 'i', 'lie', 'to', 'a', 'start', 'in', 'a', 'beautiful', 'world', 'yeah', 'we', \"how'm\", 'and', 'i', 'want', 'to', 'be', 'waiting']\n",
      "Texte stochastique 2 : ['tell', 'me', \"don't\", 'and', 'if', 'things', 'which', 'way', 'home', '', 'oh', 'no', 'light', 'a', 'river', 'that', 'occurs', 'they', 'need', 'to', 'discover', 'you']\n",
      "Texte stochastique 3 : ['tell', 'me', '', '', 'when', 'you', 'as', 'me', 'decide', 'gets', 'you', 'wanted', 'and', 'the', 'room', 'as', 'good', 'as', 'mine', 'as', 'good', 'as']\n",
      "Texte max : ['tell', 'me', '', '', 'oh', 'no', 'i', 'never', 'meant', 'to', 'do', 'you', 'know', 'that', 'i', 'shoot', 'across', 'the', 'sun', 'or', 'write', 'a']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Réseau de neurones récurrent, modèle de langue par mot, pour paroles de chansons\n",
    "Fichier ColdPlay.csv de https://www.kaggle.com/datasets/deepshah16/song-lyrics-dataset\n",
    "Version avec couche vectorisation de mots et RNN\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # Pour résultats reproductibles\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "taille_sequence = 8\n",
    "\n",
    "# Déterminer si un GPU est disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Entrainement sur ',device)\n",
    "\n",
    "class DatasetParoles(torch.utils.data.Dataset):\n",
    "    \"\"\" Créer un Dataset avec les paroles de la colonne Lyric du fichier nom_fichier\n",
    "    taille_sequence : taille d'une séquence de mots pour le modèle de langue\n",
    "    Le texte est découpé en séquences de la taille taille_sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, nom_fichier, taille_sequence=4):\n",
    "        self.nom_fichier = nom_fichier\n",
    "        self.taille_sequence = taille_sequence\n",
    "        self.mots = self.charger_mots()\n",
    "        self.mots_uniques = self.chercher_mots_uniques()\n",
    "\n",
    "        self.index_a_mot = {index: mot for index, mot in enumerate(self.mots_uniques)}\n",
    "        self.mot_a_index = {mot: index for index, mot in enumerate(self.mots_uniques)}\n",
    "\n",
    "        self.mots_indexes = [self.mot_a_index[w] for w in self.mots]\n",
    "\n",
    "    def charger_mots(self):\n",
    "        dataframe_entrainement = pd.read_csv(self.nom_fichier)\n",
    "        texte_concatene = dataframe_entrainement['Lyric'].str.cat(sep=' ')\n",
    "        return texte_concatene.split(' ')\n",
    "\n",
    "    def chercher_mots_uniques(self):\n",
    "        frequence_mot = Counter(self.mots)\n",
    "        return sorted(frequence_mot, key=frequence_mot.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mots_indexes) - self.taille_sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.mots_indexes[index:index+self.taille_sequence]),\n",
    "            torch.tensor(self.mots_indexes[index+1:index+self.taille_sequence+1]),\n",
    "        )\n",
    "    \n",
    "class Modele(nn.Module):\n",
    "    \"\"\"Modèle de RNR avec une couche vectorisation, suivie d'une couche RNN et d'une couche linéaire\"\"\"\n",
    "    def __init__(self, ds_paroles):\n",
    "        super(Modele, self).__init__()\n",
    "        self.taille_H_RNN = 128\n",
    "        self.taille_vectorisation_mots = 64\n",
    "        self.nombre_couches_RNR = 1\n",
    "\n",
    "        taille_vocabulaire = len(ds_paroles.mots_uniques)\n",
    "        self.vectorisation_mots = nn.Embedding(num_embeddings=taille_vocabulaire,\n",
    "            embedding_dim=self.taille_vectorisation_mots)\n",
    "        self.rnn = nn.RNN(input_size=self.taille_vectorisation_mots,hidden_size=self.taille_H_RNN,\n",
    "            num_layers=self.nombre_couches_RNR,batch_first=True)\n",
    "        self.dense_linaire = nn.Linear(self.taille_H_RNN, taille_vocabulaire)\n",
    "\n",
    "    def forward(self, lot_X, etat_0):\n",
    "        vectorisation = self.vectorisation_mots(lot_X)\n",
    "        lot_Ht, etat = self.rnn(vectorisation, etat_0)\n",
    "        lot_Yt = self.dense_linaire(lot_Ht)\n",
    "        return lot_Yt, etat\n",
    "\n",
    "    def initializer_etat(self, taille_sequence):\n",
    "        return (torch.zeros(self.nombre_couches_RNR, taille_sequence, self.taille_H_RNN))\n",
    "\n",
    "ds_paroles = DatasetParoles(\"ColdPlay.csv\",taille_sequence=taille_sequence)\n",
    "modele = Modele(ds_paroles)\n",
    "# Placer le modèle en mode GPU si possible\n",
    "modele = modele.to(device)\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "def entrainer_RNR(ds_paroles, modele, taille_lot=32, epochs=5, taille_sequence=6):\n",
    "    debut = time.time()\n",
    "    modele.train()\n",
    "    dl_paroles = DataLoader(ds_paroles,batch_size=taille_lot)\n",
    "\n",
    "    fonction_cout = nn.CrossEntropyLoss()\n",
    "    optimizeur = optim.Adam(modele.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for lot, (lot_X, lot_Y) in enumerate(dl_paroles):\n",
    "            lot_X = lot_X.to(device)\n",
    "            lot_Y = lot_Y.to(device)\n",
    "            etat = modele.initializer_etat(lot_X.shape[0])\n",
    "            etat = etat.to(device)\n",
    "            optimizeur.zero_grad()\n",
    "            \n",
    "            lot_Y_predictions, etat = modele(lot_X, etat)\n",
    "            cout = fonction_cout(lot_Y_predictions.transpose(1, 2), lot_Y)\n",
    "            \n",
    "            cout.backward()\n",
    "            optimizeur.step()\n",
    "            if lot%100 == 0:\n",
    "                print(f'-------- > epoch {epoch} lot {lot} :  coût = {cout.item()}')\n",
    "                temps_ecoule = time.time() - debut\n",
    "                print('Temps écoulé : {:.0f}m {:.0f}s'.format(temps_ecoule // 60, temps_ecoule % 60))\n",
    "\n",
    "\n",
    "def predire_stochastique(ds, modele, debut_texte, nb_mots=20):\n",
    "    \"\"\" Prédire une suite de nb_mots à partir de debut_texte selon le modele\"\"\"\n",
    "    mots = debut_texte.split(' ')\n",
    "    modele.eval()\n",
    "    etat = modele.initializer_etat(1)\n",
    "    etat = etat.to(device)\n",
    "    for i in range(0, nb_mots):\n",
    "        lot_X = torch.tensor([[ds.mot_a_index[m] for m in mots[i:]]])\n",
    "        lot_X = lot_X.to(device)\n",
    "        lot_Y_predictions, etat = modele(lot_X, etat)\n",
    "        dernier_mot_Yt = lot_Y_predictions[0][-1]\n",
    "        probs_dernier_mot = torch.nn.functional.softmax(dernier_mot_Yt, dim=0).data\n",
    "        #index_mot_choisi = torch.max(probs_dernier_mot, dim=0)[1].item()\n",
    "        index_mot_choisi = torch.multinomial(probs_dernier_mot, 1)[0].item()\n",
    "        mots.append(ds.index_a_mot[index_mot_choisi])\n",
    "    return mots\n",
    "\n",
    "def predire(ds, modele, debut_texte, nb_mots=20, mode =0):\n",
    "    \"\"\" Prédire une suite de nb_mots à partir de debut_texte selon le modele\"\"\"\n",
    "\n",
    "    mots = debut_texte.split(' ')\n",
    "    modele.eval()\n",
    "    etat = modele.initializer_etat(1)\n",
    "    etat = etat.to(device)\n",
    "    for i in range(0, nb_mots):\n",
    "        lot_X = torch.tensor([[ds.mot_a_index[m] for m in mots[i:]]])\n",
    "        lot_X = lot_X.to(device)\n",
    "        lot_Y_predictions, etat = modele(lot_X, etat)\n",
    "        dernier_mot_Yt = lot_Y_predictions[0][-1]\n",
    "        probs_dernier_mot = torch.nn.functional.softmax(dernier_mot_Yt, dim=0).data\n",
    "        if mode == 0 :\n",
    "            index_mot_choisi = torch.max(probs_dernier_mot, dim=0)[1].item()\n",
    "        else :\n",
    "            index_mot_choisi = torch.multinomial(probs_dernier_mot, 1)[0].item()\n",
    "        mots.append(ds.index_a_mot[index_mot_choisi])\n",
    "    return mots\n",
    "\n",
    "entrainer_RNR(ds_paroles, modele, taille_lot=32, epochs=5, taille_sequence=taille_sequence)\n",
    "print(\"Texte stochastique 1 :\",predire(ds_paroles, modele, debut_texte='tell me', mode = 1))\n",
    "print(\"Texte stochastique 2 :\",predire(ds_paroles, modele, debut_texte='tell me', mode = 1))\n",
    "print(\"Texte stochastique 3 :\",predire(ds_paroles, modele, debut_texte='tell me', mode = 1))\n",
    "print(\"Texte max :\",predire(ds_paroles, modele, debut_texte='tell me', mode = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
