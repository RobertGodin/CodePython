{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- > epoch 1:  erreur quadratique moyenne 0.15943032250293515\n",
      "-------- > epoch 2:  erreur quadratique moyenne 0.09925017200618305\n",
      "-------- > epoch 3:  erreur quadratique moyenne 0.08504018042834774\n",
      "-------- > epoch 4:  erreur quadratique moyenne 0.07672600484904755\n",
      "-------- > epoch 5:  erreur quadratique moyenne 0.07115550154773981\n",
      "-------- > epoch 6:  erreur quadratique moyenne 0.06705372524024171\n",
      "-------- > epoch 7:  erreur quadratique moyenne 0.06379094774576705\n",
      "-------- > epoch 8:  erreur quadratique moyenne 0.06109168886395831\n",
      "-------- > epoch 9:  erreur quadratique moyenne 0.058422963278604904\n",
      "-------- > epoch 10:  erreur quadratique moyenne 0.05616649676567379\n",
      "-------- > epoch 11:  erreur quadratique moyenne 0.05451841761229951\n",
      "-------- > epoch 12:  erreur quadratique moyenne 0.052799982622557716\n",
      "-------- > epoch 13:  erreur quadratique moyenne 0.051392193906160566\n",
      "-------- > epoch 14:  erreur quadratique moyenne 0.049731356211714976\n",
      "-------- > epoch 15:  erreur quadratique moyenne 0.0483177600512491\n",
      "-------- > epoch 16:  erreur quadratique moyenne 0.04743754770415796\n",
      "-------- > epoch 17:  erreur quadratique moyenne 0.04624023708709568\n",
      "-------- > epoch 18:  erreur quadratique moyenne 0.044974666065610146\n",
      "-------- > epoch 19:  erreur quadratique moyenne 0.04403216036689757\n",
      "-------- > epoch 20:  erreur quadratique moyenne 0.04334314381420978\n",
      "-------- > epoch 21:  erreur quadratique moyenne 0.0426330294111794\n",
      "-------- > epoch 22:  erreur quadratique moyenne 0.042020044357229155\n",
      "-------- > epoch 23:  erreur quadratique moyenne 0.04142656623183485\n",
      "-------- > epoch 24:  erreur quadratique moyenne 0.04069057451348755\n",
      "-------- > epoch 25:  erreur quadratique moyenne 0.03993059402386903\n",
      "-------- > epoch 26:  erreur quadratique moyenne 0.03882604993229434\n",
      "-------- > epoch 27:  erreur quadratique moyenne 0.03827552633970277\n",
      "-------- > epoch 28:  erreur quadratique moyenne 0.03782965678907838\n",
      "-------- > epoch 29:  erreur quadratique moyenne 0.03664782772451215\n",
      "-------- > epoch 30:  erreur quadratique moyenne 0.03650587680346522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x198865a6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x198940aa940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe de l'image 0 : [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] Prédiction: [array([[2.21711993e-04, 3.12718833e-06, 4.28797503e-07, 1.36692745e-01,\n",
      "        4.39401122e-13, 7.79773057e-01, 1.23243777e-09, 2.66546576e-06,\n",
      "        1.90232225e-08, 2.69132998e-03]])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mpl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8748ed61bb49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mimage_applatie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdonnees_ent_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0mune_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_applatie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mune_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"nearest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpl' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "# Implémentation d'un RNA par couche\n",
    "# Deux types de couches : dense linéaire et activation\n",
    "# Division des données en deux groupes : entrainement et test\n",
    "# Initialisation He\n",
    "\n",
    "# Exemple avec MNIST\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42) # pour reproduire les mêmes résultats\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "class Couche:\n",
    "    \"\"\" Classe abstraite qui représente une couche du RNA\n",
    "        X:  np.array 2D de taille (1,n), entrée de la couche \n",
    "        Y: np.array 2D de taille (1,m), sortie de la couche\n",
    "    \"\"\"\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Calculer la sortie Y pour une valeur de X\n",
    "        \n",
    "        X : vecteur des variables prédictives\n",
    "        Les valeurs de X et Y sont stockées pour les autres traitements.\n",
    "        \"\"\"\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées par rapport à X et les autres paramètres à partir de dJ_dY\n",
    "        et mettre à jour les paramètres de la couche selon le taux spécifié.\n",
    "        \n",
    "        dJ_dY : np.array(1,m), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "\n",
    "class CoucheDenseLineaire(Couche):\n",
    "    \"\"\" Couche linéaire dense. Y=WX+B\n",
    "    \"\"\"\n",
    "    def __init__(self,n,m,init_W=None,init_B=None):\n",
    "        \"\"\" Initilalise les paramètres de la couche. W et B sont initialisés avec init_W et init_B lorsque spécifiés.\n",
    "        Sinon, des valeurs aléatoires sont générés pour W une distribution normale et B est initialisée avec des 0 \n",
    "        si les paramètres init_W et init_B ne sont pas spécifiés.\n",
    "        L'initialization He est employée pour W\n",
    "        n : int, taille du vecteur d'entrée X\n",
    "        m : int, taille du vecteur de sortie Y\n",
    "        init_W : np.array, shape(n,m), valeur initiale optionnelle de W\n",
    "        init_B : np.array, shape(1,m), valeur initial optionnelle de B\n",
    "        \"\"\"\n",
    "        if init_W is None :\n",
    "            # Initialization He\n",
    "            self.W = np.random.randn(n,m) * np.sqrt(2/n) \n",
    "        else:\n",
    "            self.W = init_W\n",
    "        if init_B is None :\n",
    "            self.B = np.zeros((1,m))\n",
    "        else:\n",
    "            self.B = init_B\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Fait la propagation de X et retourne Y=WX+B. \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.B + np.dot(self.X,self.W)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées dJ_dW,dJ_dB,dJ_dX pour une couche dense linéaire et\n",
    "        mettre à jour les paramètres selon le taux spécifié\n",
    "        \n",
    "        dJ_dY : np.array(1,2), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "        dJ_dW = np.dot(self.X.T,dJ_dY)\n",
    "        dJ_dB = dJ_dY\n",
    "        dJ_dX = np.dot(dJ_dY,self.W.T)\n",
    "        if trace:\n",
    "            print(\"dJ_dW:\",dJ_dW)\n",
    "            print(\"dJ_dB:\",dJ_dB)\n",
    "            print(\"dJ_dX:\",dJ_dX)\n",
    "        # Metre à jour les paramètres W et B\n",
    "        self.W -= taux * dJ_dW\n",
    "        self.B -= taux * dJ_dB\n",
    "        if trace:\n",
    "            print(\"W modifié:\",self.W)\n",
    "            print(\"B modifié:\",self.B)\n",
    "        return dJ_dX\n",
    "\n",
    "    \n",
    "class CoucheActivation(Couche):\n",
    "    \"\"\" Couche d'activation selon une fonction spécifiée dans le constructeur\n",
    "    \"\"\"\n",
    "    def __init__(self,fonction_activation,derivee):\n",
    "        \"\"\" Initialise la fonction_activation ainsi que la dérivée\n",
    "        fonction_activation: une fonction qui prend chacune des valeurs de X et \n",
    "        retourne Y=fonction_activation(X)\n",
    "        derivee: une fonction qui calcule la dérivée la fonction_activation\n",
    "        \"\"\"\n",
    "        self.fonction_activation = fonction_activation\n",
    "        self.derivee = derivee\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Retourne Y=fonction_activation(X)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.fonction_activation(self.X)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Retourne la dérivée de la fonction d'activation par rapport l'entrée X\n",
    "        Le taux n'est pas utilisé parce qu'il n'y a pas de paramètres à modifier dans ce genre de couche\n",
    "        \"\"\"\n",
    "        return self.derivee(self.X) * dJ_dY\n",
    "\n",
    "def erreur_quadratique(y_prediction,y):\n",
    "    \"\"\" Retourne l'erreur quadratique entre la prédiction y_prediction et la valeur attendue y\n",
    "    \"\"\" \n",
    "    return np.sum(np.power(y_prediction-y,2))\n",
    "\n",
    "\n",
    "def d_erreur_quadratique(y_prediction,y):\n",
    "    return 2*(y_prediction-y)\n",
    "\n",
    "class ReseauMultiCouches:\n",
    "    \"\"\" Réseau mutli-couche formé par une séquence de Couches\n",
    "    \n",
    "    couches : liste de Couches du RNA\n",
    "    cout : fonction qui calcule de cout J\n",
    "    derivee_cout: dérivée de la fonction de cout\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.couches = []\n",
    "        self.cout = None\n",
    "        self.derivee_cout = None\n",
    "\n",
    "    def ajouter_couche(self,couche):\n",
    "        self.couches.append(couche)\n",
    "\n",
    "    def specifier_J(self,cout,derivee_cout):\n",
    "        \"\"\" Spécifier la fonction de coût J et sa dérivée\n",
    "        \"\"\"\n",
    "        self.cout = cout\n",
    "        self.derivee_cout = derivee_cout\n",
    "\n",
    "    def propagation_donnees_X(self,donnees_X,trace=False):\n",
    "        \"\"\" Prédire Y pour chacune des observations dans donnees_X)\n",
    "        donnees_X : np.array 3D des valeurs de X pour chacune des observations\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_X)\n",
    "        predictions_Y = []\n",
    "        for indice_observation in range(nb_observations):\n",
    "            # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "            # à la valeur de Y de la couche précédente\n",
    "            XY_propage = donnees_X[indice_observation]\n",
    "            if trace: \n",
    "                print(\"Valeur de X initiale:\",XY_propage)\n",
    "            for couche in self.couches:\n",
    "                XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                if trace: \n",
    "                    print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "            predictions_Y.append(XY_propage)\n",
    "\n",
    "        return predictions_Y\n",
    "\n",
    "    def metriques(self, donnees_X,donnees_Y):\n",
    "        \"\"\"Retourne le cout moyen, la proportion de bons résultats\n",
    "        Choisit l'indice de la classe dont l'activation est la plus grande\"\"\"\n",
    "        erreur_quadratique = 0\n",
    "        nb_correct = 0\n",
    "        predictions_Y=self.propagation_donnees_X(donnees_X)\n",
    "        for indice in range(len(donnees_Y)):\n",
    "            erreur_quadratique += self.cout(predictions_Y[indice],donnees_Y[indice])\n",
    "            classe_predite = np.argmax(predictions_Y[indice])\n",
    "            if donnees_Y[indice][0,classe_predite] == 1:\n",
    "                nb_correct+=1\n",
    "        return (erreur_quadratique/len(donnees_Y),nb_correct/len(donnees_Y))\n",
    "\n",
    "    def entrainer_descente_gradiant_stochastique(self,donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                 nb_epochs,taux,trace=False,graph_cout=False):\n",
    "        \"\"\" Entrainer le réseau par descente de gradiant stochastique (une observation à la fois)\n",
    "        \n",
    "        donnees_ent_X : np.array 3D des valeurs de X pour chacune des observations d'entrainement\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_ent_Y : np.array 3D des valeurs de Y pour chacune des observations d'entrainement\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        donnees_test_X : np.array 3D des valeurs de X pour chacune des observations de test\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_test_Y : np.array 3D des valeurs de Y pour chacune des observations de test\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        nb_epochs : nombre de cycle de passage sur les données d'entainement\n",
    "        taux : taux dans la descente de gradiant\n",
    "        trace : Boolean, True pour afficher une trace des calculs effectués sur les paramètres\n",
    "        graph_cout : Boolean, True pur afficher un graphique de l'évolution du coût\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_ent_X)\n",
    "        if graph_cout :\n",
    "            liste_cout_moyen_ent = []\n",
    "            liste_ok_ent = []\n",
    "            liste_cout_moyen_test = []\n",
    "            liste_ok_test = []\n",
    "\n",
    "        # Boucle d'entrainement principale, nb_epochs fois\n",
    "        for cycle in range(nb_epochs):\n",
    "            cout_total = 0\n",
    "            # Descente de gradiant stochastique, une observation à la fois\n",
    "            for indice_observation in range(nb_observations):\n",
    "                # Propagation avant pour une observation X\n",
    "                # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "                # à la valeur de Y de la couche précédente\n",
    "                XY_propage = donnees_ent_X[indice_observation]\n",
    "                if trace: \n",
    "                    print(\"Valeur de X initiale:\",XY_propage)\n",
    "\n",
    "                for couche in self.couches:\n",
    "                    XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                    if trace: \n",
    "                        print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "\n",
    "                # Calcul du coût pour une observation\n",
    "                cout_total += self.cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "\n",
    "                # Rétropropagation pour une observation\n",
    "                # dJ_dX_dJ_dY représente la valeur de la dérivée dJ_dX de la couche suivante\n",
    "                # qui correspond à dJ_dY de la couche en cours de traitement\n",
    "                dJ_dX_dJ_dY = self.derivee_cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "                if trace :\n",
    "                    print(\"dJ_dY pour la couche finale:\",dJ_dX_dJ_dY)\n",
    "                for couche in reversed(self.couches):\n",
    "                    dJ_dX_dJ_dY = couche.retropropager_une_couche(dJ_dX_dJ_dY,taux,trace)\n",
    "\n",
    "            # Calculer et afficher le coût moyen pour une epoch\n",
    "            cout_moyen = cout_total/nb_observations\n",
    "            if graph_cout:\n",
    "                print(f'-------- > epoch {cycle+1}:  erreur quadratique moyenne {cout_moyen}')\n",
    "                cout_ent,ok_ent = self.metriques(donnees_ent_X,donnees_ent_Y)\n",
    "                cout_test,ok_test = self.metriques(donnees_test_X,donnees_test_Y)\n",
    "                liste_cout_moyen_ent.append(cout_ent)\n",
    "                liste_ok_ent.append(ok_ent)\n",
    "                liste_cout_moyen_test.append(cout_test)\n",
    "                liste_ok_test.append(ok_test)\n",
    "                \n",
    "            \n",
    "        # Affichage du graphique d'évolution de l'erreur quadratique\n",
    "        if graph_cout:\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_ent,label='Erreur entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_test,label='Erreur test')\n",
    "            plt.title(\"Evolution de L'erreur quadratique\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_ent,label='entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_test,label='test')\n",
    "            plt.title(\"Evolution du taux de bonnes prédictions\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivee_tanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoide(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def derivee_sigmoide(x):\n",
    "    return sigmoide(x)*(1-sigmoide(x))\n",
    "\n",
    "\n",
    "def bitmap(classe):\n",
    "    \"\"\" Representer l'entier de classe par un vecteur bitmap (10,1) \n",
    "    classe : entier ebitmap(ntre 0 et 9 qui représente la classe de l'observation\"\"\"\n",
    "    e = np.zeros((1,10))\n",
    "    e[0,classe] = 1.0\n",
    "    return e\n",
    "\n",
    "# Chargement des données de MNIST\n",
    "import pickle, gzip\n",
    "\n",
    "fichier_donnees = gzip.open(r\"mnist.pkl.gz\", 'rb')\n",
    "donnees_ent, donnees_validation, donnees_test = pickle.load(fichier_donnees, encoding='latin1')\n",
    "fichier_donnees.close()\n",
    "    \n",
    "donnees_ent_X = donnees_ent[0].reshape((50000,1,784))\n",
    "donnees_ent_Y = [bitmap(y) for y in donnees_ent[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "donnees_test_X = donnees_test[0].reshape((10000,1,784))\n",
    "donnees_test_Y = [bitmap(y) for y in donnees_test[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "\n",
    "# Définir l'architecture du RNA \n",
    "# Deux couches denses linéaires suivies chacune d'une couche d'activation sigmoide\n",
    "un_RNA = ReseauMultiCouches()\n",
    "un_RNA.specifier_J(erreur_quadratique,d_erreur_quadratique)\n",
    "un_RNA.ajouter_couche(CoucheDenseLineaire(784,30))\n",
    "un_RNA.ajouter_couche(CoucheActivation(sigmoide,derivee_sigmoide))\n",
    "un_RNA.ajouter_couche(CoucheDenseLineaire(30,10))\n",
    "un_RNA.ajouter_couche(CoucheActivation(sigmoide,derivee_sigmoide))\n",
    "# Entrainer le RNA\n",
    "un_RNA.entrainer_descente_gradiant_stochastique(donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                nb_epochs=30,taux=0.1,trace = False, graph_cout = True)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Classe de l'image\",i,\":\",donnees_ent_Y[i],\"Prédiction:\",un_RNA.propagation_donnees_X(donnees_ent_X[i]))\n",
    "    image_applatie = donnees_ent_X[i]\n",
    "    une_image = image_applatie.reshape(28, 28)\n",
    "    plt.imshow(une_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
