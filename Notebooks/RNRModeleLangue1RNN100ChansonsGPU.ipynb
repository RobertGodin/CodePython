{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement sur  cpu\n",
      "-------- > epoch 0 lot 0 :  coût = 8.427059173583984\n",
      "Temps écoulé : 0m 0s\n",
      "-------- > epoch 0 lot 100 :  coût = 7.852601528167725\n",
      "Temps écoulé : 0m 6s\n",
      "-------- > epoch 0 lot 200 :  coût = 5.910061359405518\n",
      "Temps écoulé : 0m 12s\n",
      "-------- > epoch 0 lot 300 :  coût = 7.570687294006348\n",
      "Temps écoulé : 0m 19s\n",
      "-------- > epoch 0 lot 400 :  coût = 7.217875003814697\n",
      "Temps écoulé : 0m 26s\n",
      "-------- > epoch 0 lot 500 :  coût = 8.715989112854004\n",
      "Temps écoulé : 0m 32s\n",
      "-------- > epoch 0 lot 600 :  coût = 7.0795793533325195\n",
      "Temps écoulé : 0m 39s\n",
      "-------- > epoch 1 lot 0 :  coût = 5.712224960327148\n",
      "Temps écoulé : 0m 39s\n",
      "-------- > epoch 1 lot 100 :  coût = 6.062659740447998\n",
      "Temps écoulé : 0m 45s\n",
      "-------- > epoch 1 lot 200 :  coût = 4.858207702636719\n",
      "Temps écoulé : 0m 51s\n",
      "-------- > epoch 1 lot 300 :  coût = 6.471259117126465\n",
      "Temps écoulé : 0m 57s\n",
      "-------- > epoch 1 lot 400 :  coût = 6.080842018127441\n",
      "Temps écoulé : 1m 4s\n",
      "-------- > epoch 1 lot 500 :  coût = 7.5885210037231445\n",
      "Temps écoulé : 1m 10s\n",
      "-------- > epoch 1 lot 600 :  coût = 5.2502336502075195\n",
      "Temps écoulé : 1m 16s\n",
      "-------- > epoch 2 lot 0 :  coût = 4.131104946136475\n",
      "Temps écoulé : 1m 16s\n",
      "-------- > epoch 2 lot 100 :  coût = 4.796230316162109\n",
      "Temps écoulé : 1m 23s\n",
      "-------- > epoch 2 lot 200 :  coût = 3.791975259780884\n",
      "Temps écoulé : 1m 29s\n",
      "-------- > epoch 2 lot 300 :  coût = 5.695844650268555\n",
      "Temps écoulé : 1m 35s\n",
      "-------- > epoch 2 lot 400 :  coût = 5.225996017456055\n",
      "Temps écoulé : 1m 42s\n",
      "-------- > epoch 2 lot 500 :  coût = 6.4000115394592285\n",
      "Temps écoulé : 1m 48s\n",
      "-------- > epoch 2 lot 600 :  coût = 3.421703815460205\n",
      "Temps écoulé : 1m 55s\n",
      "-------- > epoch 3 lot 0 :  coût = 2.8140227794647217\n",
      "Temps écoulé : 1m 55s\n",
      "-------- > epoch 3 lot 100 :  coût = 3.416497230529785\n",
      "Temps écoulé : 2m 2s\n",
      "-------- > epoch 3 lot 200 :  coût = 2.87528395652771\n",
      "Temps écoulé : 2m 8s\n",
      "-------- > epoch 3 lot 300 :  coût = 4.951777935028076\n",
      "Temps écoulé : 2m 15s\n",
      "-------- > epoch 3 lot 400 :  coût = 4.505502700805664\n",
      "Temps écoulé : 2m 21s\n",
      "-------- > epoch 3 lot 500 :  coût = 5.222965717315674\n",
      "Temps écoulé : 2m 27s\n",
      "-------- > epoch 3 lot 600 :  coût = 2.2656188011169434\n",
      "Temps écoulé : 2m 34s\n",
      "-------- > epoch 4 lot 0 :  coût = 1.9291348457336426\n",
      "Temps écoulé : 2m 34s\n",
      "-------- > epoch 4 lot 100 :  coût = 2.3415255546569824\n",
      "Temps écoulé : 2m 40s\n",
      "-------- > epoch 4 lot 200 :  coût = 2.2059030532836914\n",
      "Temps écoulé : 2m 46s\n",
      "-------- > epoch 4 lot 300 :  coût = 4.336151123046875\n",
      "Temps écoulé : 2m 53s\n",
      "-------- > epoch 4 lot 400 :  coût = 3.855116367340088\n",
      "Temps écoulé : 2m 60s\n",
      "-------- > epoch 4 lot 500 :  coût = 4.069504737854004\n",
      "Temps écoulé : 3m 6s\n",
      "-------- > epoch 4 lot 600 :  coût = 1.6250028610229492\n",
      "Temps écoulé : 3m 13s\n",
      "['There', 'was', 'end', 'at', 'come.', 'Everyday', 'is', 'a', 'caution,.', 'was', 'raw', 'the', 'pavement.', 'she', 'snake', 'the', 'grave,.', 'below..', 'Jenny.', 'Jenny', 'you', 'conceal']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Réseau de neurones récurrent, modèle de langue par mot, pour paroles de chansons\n",
    "Version avec couche vectorisation de mots et RNN \n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # Pour résultats reproductibles\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "taille_sequence = 8\n",
    "\n",
    "# Déterminer si un GPU est disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Entrainement sur ',device)\n",
    "\n",
    "class DatasetParoles(torch.utils.data.Dataset):\n",
    "    \"\"\" Créer un Dataset avec les paroles de la colonne Lyric du fichier \n",
    "    https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres?select=lyrics-data.csv\n",
    "    taille_sequence : taille d'une séquence de mots pour le modèle de langue\n",
    "    Le texte est découpé en séquences de la taille taille_sequence\n",
    "    \"\"\"\n",
    "    def __init__(self,taille_sequence=4):\n",
    "        self.taille_sequence = taille_sequence\n",
    "        self.mots = self.charger_mots()\n",
    "        self.mots_uniques = self.chercher_mots_uniques()\n",
    "\n",
    "        self.index_a_mot = {index: mot for index, mot in enumerate(self.mots_uniques)}\n",
    "        self.mot_a_index = {mot: index for index, mot in enumerate(self.mots_uniques)}\n",
    "\n",
    "        self.mots_indexes = [self.mot_a_index[w] for w in self.mots]\n",
    "\n",
    "    def charger_mots(self):\n",
    "        dataframe_entrainement = pd.read_csv('lyrics-data.csv')\n",
    "        texte_concatene = dataframe_entrainement.iloc[0:100]['Lyric'].str.cat(sep=' ')\n",
    "        return texte_concatene.split(' ')\n",
    "\n",
    "    def chercher_mots_uniques(self):\n",
    "        frequence_mot = Counter(self.mots)\n",
    "        return sorted(frequence_mot, key=frequence_mot.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mots_indexes) - self.taille_sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.mots_indexes[index:index+self.taille_sequence]),\n",
    "            torch.tensor(self.mots_indexes[index+1:index+self.taille_sequence+1]),\n",
    "        )\n",
    "    \n",
    "class Modele(nn.Module):\n",
    "    \"\"\"Modèle de RNR avec une couche vectorisation, suivie d'une couche RNN et d'une couche linéaire\"\"\"\n",
    "    def __init__(self, ds_paroles):\n",
    "        super(Modele, self).__init__()\n",
    "        self.taille_H_RNN = 128\n",
    "        self.taille_vectorisation_mots = 64\n",
    "        self.nombre_couches_RNR = 1\n",
    "\n",
    "        taille_vocabulaire = len(ds_paroles.mots_uniques)\n",
    "        self.vectorisation_mots = nn.Embedding(num_embeddings=taille_vocabulaire,\n",
    "            embedding_dim=self.taille_vectorisation_mots)\n",
    "        self.rnn = nn.RNN(input_size=self.taille_vectorisation_mots,hidden_size=self.taille_H_RNN,\n",
    "            num_layers=self.nombre_couches_RNR,batch_first=True)\n",
    "        self.dense_linaire = nn.Linear(self.taille_H_RNN, taille_vocabulaire)\n",
    "\n",
    "    def forward(self, lot_X, etat_0):\n",
    "        vectorisation = self.vectorisation_mots(lot_X)\n",
    "        lot_Ht, etat = self.rnn(vectorisation, etat_0)\n",
    "        lot_Yt = self.dense_linaire(lot_Ht)\n",
    "        return lot_Yt, etat\n",
    "\n",
    "    def initializer_etat(self, taille_sequence):\n",
    "        return (torch.zeros(self.nombre_couches_RNR, taille_sequence, self.taille_H_RNN))\n",
    "\n",
    "ds_paroles = DatasetParoles(taille_sequence=taille_sequence)\n",
    "modele = Modele(ds_paroles)\n",
    "# Placer le modèle en mode GPU si possible\n",
    "modele = modele.to(device)\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "def entrainer_RNR(ds_paroles, modele, taille_lot=32, epochs=10, taille_sequence=6):\n",
    "    debut = time.time()\n",
    "    modele.train()\n",
    "    dl_paroles = DataLoader(ds_paroles,batch_size=taille_lot)\n",
    "\n",
    "    fonction_cout = nn.CrossEntropyLoss()\n",
    "    optimizeur = optim.Adam(modele.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for lot, (lot_X, lot_Y) in enumerate(dl_paroles):\n",
    "            lot_X = lot_X.to(device)\n",
    "            lot_Y = lot_Y.to(device)\n",
    "            etat = modele.initializer_etat(lot_X.shape[0])\n",
    "            etat = etat.to(device)\n",
    "            optimizeur.zero_grad()\n",
    "            \n",
    "            lot_Y_predictions, etat = modele(lot_X, etat)\n",
    "            cout = fonction_cout(lot_Y_predictions.transpose(1, 2), lot_Y)\n",
    "            \n",
    "            cout.backward()\n",
    "            optimizeur.step()\n",
    "            if lot%100 == 0:\n",
    "                print(f'-------- > epoch {epoch} lot {lot} :  coût = {cout.item()}')\n",
    "                temps_ecoule = time.time() - debut\n",
    "                print('Temps écoulé : {:.0f}m {:.0f}s'.format(temps_ecoule // 60, temps_ecoule % 60))\n",
    "\n",
    "\n",
    "def predire(ds, modele, debut_texte, nb_mots=20):\n",
    "    \"\"\" Prédire une suite de nb_mots à partir de debut_texte selon le modele\"\"\"\n",
    "    mots = debut_texte.split(' ')\n",
    "    modele.eval()\n",
    "    etat = modele.initializer_etat(1)\n",
    "    etat = etat.to(device)\n",
    "    for i in range(0, nb_mots):\n",
    "        lot_X = torch.tensor([[ds.mot_a_index[m] for m in mots[i:]]])\n",
    "        lot_X = lot_X.to(device)\n",
    "        lot_Y_predictions, etat = modele(lot_X, etat)\n",
    "        dernier_mot_Yt = lot_Y_predictions[0][-1]\n",
    "        probs_dernier_mot = torch.nn.functional.softmax(dernier_mot_Yt, dim=0).data\n",
    "        #index_mot_choisi = torch.max(probs_dernier_mot, dim=0)[1].item()\n",
    "        index_mot_choisi = torch.multinomial(probs_dernier_mot, 1)[0].item()\n",
    "        mots.append(ds.index_a_mot[index_mot_choisi])\n",
    "    return mots\n",
    "\n",
    "entrainer_RNR(ds_paroles, modele, taille_lot=32, epochs=5, taille_sequence=taille_sequence)\n",
    "print(predire(ds_paroles, modele, debut_texte='There was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
