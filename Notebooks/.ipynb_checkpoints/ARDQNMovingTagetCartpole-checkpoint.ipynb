{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/200. Longueur:912"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "afficher_longueur_episode() got an unexpected keyword argument 'fenetre'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-03ac42dfd749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m \u001b[0mafficher_longueur_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongueur_episode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfenetre\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: afficher_longueur_episode() got an unexpected keyword argument 'fenetre'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Deep Q-learning avec cibles-Q fixes\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "random.seed(42)\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "Observation = namedtuple('Observation',\n",
    "                        ('etat', 'action', 'etat_suivant', 'recompense'))\n",
    "class HistoriqueObservations(object):\n",
    "    \"\"\" Stockage de la liste des observations pour l'apprentissage \n",
    "        Mémoire circulaire pour limiter à taille_memoire\"\"\"\n",
    "    def __init__(self, taille_memoire):\n",
    "        self.taille_memoire = taille_memoire\n",
    "        self.indice_courant = 0\n",
    "        self.liste_observations = []\n",
    "\n",
    "\n",
    "    def ajouter_historique(self, *args):\n",
    "        \"\"\"AJoutee une observation. Écriture circulaire.\"\"\"\n",
    "        if len(self.liste_observations) < self.taille_memoire:\n",
    "            self.liste_observations.append(None)\n",
    "        self.liste_observations[self.indice_courant] = Observation(*args)\n",
    "        self.indice_courant = (self.indice_courant + 1) % self.taille_memoire\n",
    "\n",
    "    def mini_lot_observations(self, taille_mini_lot):\n",
    "        return random.sample(self.liste_observations, taille_mini_lot)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.liste_observations)\n",
    "    \n",
    "class RNAQ(nn.Module):\n",
    "    \"\"\"Prend un état en entrée et produit la prédiction de Q pour chacune des actions\n",
    "    l'état est une représentation de l'écran (carré centré sur le chariot, différence entre deux images)\n",
    "    3 canaux en entrée\n",
    "    hauteur : hauteur de l'image en entrée\n",
    "    largeur : largeur de limage\n",
    "    nb_actions_y : le nombre d'actions possibles pour l'environnement\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hauteur, largeur, nb_actions_y):\n",
    "        super(RNAQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) # 3 canaux en entrée\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def taille_sortie_convolution(taille_h_ou_l, kernel_size = 5, stride = 2):\n",
    "            \"\"\" Calculer la taille de la sortie en fonction des paramètres de la convolution\n",
    "                taille_h_ou_l: taille de l'entrée (largeur ou hauteur de l'image)\n",
    "            \"\"\"\n",
    "            return (taille_h_ou_l - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        largeur_conv = taille_sortie_convolution(taille_sortie_convolution(taille_sortie_convolution(largeur)))\n",
    "        hauteur_conv = taille_sortie_convolution(taille_sortie_convolution(taille_sortie_convolution(hauteur)))\n",
    "        taille_X_lineaire = largeur_conv * hauteur_conv * 32\n",
    "        self.couche_finale = nn.Linear(taille_X_lineaire, nb_actions_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Retourne un mini-lot de vecteurs des probabilités pour les deux actions (gauche et droite)\"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.couche_finale(x.view(x.size(0), -1))\n",
    "\n",
    "def calculer_position_chariot(largeur_ecran):\n",
    "    largeur_monde = env.x_threshold * 2\n",
    "    echelle = largeur_ecran / largeur_monde\n",
    "    return int(env.state[0] * echelle + largeur_ecran / 2.0)  # Milieu du chariot\n",
    "\n",
    "transformer_image_ecran = T.Compose([T.ToPILImage(),T.Resize(40, interpolation=Image.CUBIC),T.ToTensor()])\n",
    "def chercher_ecran():\n",
    "    # Transposer l'image selon l'ordre (canal, largeur, hauteur)\n",
    "    ecran = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Couper le haut et le bas de l'écran\n",
    "    _, hauteur_ecran, largeur_ecran = ecran.shape\n",
    "    ecran = ecran[:, int(hauteur_ecran*0.4):int(hauteur_ecran * 0.8)]\n",
    "    largeur_vue = int(largeur_ecran * 0.6)\n",
    "    position_chariot = calculer_position_chariot(largeur_ecran)\n",
    "    if position_chariot < largeur_vue // 2:\n",
    "        portee_tranche = slice(largeur_vue)\n",
    "    elif position_chariot > (largeur_ecran - largeur_vue // 2):\n",
    "        portee_tranche = slice(-largeur_vue, None)\n",
    "    else:\n",
    "        portee_tranche = slice(position_chariot - largeur_vue // 2,\n",
    "                            position_chariot + largeur_vue // 2)\n",
    "    # Découper les marges pour obtenir un carré centré sur le chariot\n",
    "    ecran = ecran[:, :, portee_tranche]\n",
    "    ecran = np.ascontiguousarray(ecran, dtype=np.float32) / 255\n",
    "    ecran = torch.from_numpy(ecran)\n",
    "    # transformer_image_ecran, et ajouter dimension pour mini_lot (mini_lot,canal,largeur,hauteur)\n",
    "    return transformer_image_ecran(ecran).unsqueeze(0)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(chercher_ecran().cpu().squeeze(0).permute(1, 2, 0).numpy(),interpolation='none')\n",
    "plt.title(\"Exemple d'écran après transformations\")\n",
    "plt.show()\n",
    "\n",
    "# Chercher taille de l'écran pour le RNA\n",
    "ecran_initial = chercher_ecran() \n",
    "_, _, hauteur_ecran, largeur_ecran = ecran_initial.shape\n",
    "\n",
    "# Initaliser le RNAQ\n",
    "n_actions = env.action_space.n\n",
    "rnaq = RNAQ(hauteur_ecran, largeur_ecran, n_actions)\n",
    "optimiseur = optim.RMSprop(rnaq.parameters())\n",
    "liste_observations = HistoriqueObservations(10000)\n",
    "\n",
    "nombre_etapes_accomplies = 0\n",
    "\n",
    "def choisir_action(etat,epsilon):\n",
    "    \"\"\" Choisir l'action max Q (politique e-vorace) à partir de rnaq\"\"\"\n",
    "    unif_01 = random.uniform(0, 1)\n",
    "    if unif_01 > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return rnaq(etat).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]],dtype=torch.long)\n",
    "\n",
    "durees_par_episode = []\n",
    "\n",
    "def afficher_longueur_episode(longueur_episode, fenetre=10):\n",
    "    \"\"\"\n",
    "    Afficher l'évolution des longueurs d'épisodes avec le temps\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(longueur_episode,label=\"Longueur épisode\")\n",
    "    longueur_moyenne_fenetre=[longueur_episode[i:i+fenetre].mean() for i in range(longueur_episode.shape[0]-fenetre)]\n",
    "    plt.plot(np.arange(fenetre,longueur_episode.shape[0]),longueur_moyenne_fenetre,label=\"Moyenne mobile\")\n",
    "    plt.xlabel(\"Épisode\")\n",
    "    plt.ylabel(\"Longueur épisode\")\n",
    "    plt.title(\"DQN (Deep Q-learning), Cartpole : évolution de la longueur d'épisode, fenetre:\"+str(fenetre))\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def optimisation_RNAQ(taille_mini_lot = 128, gamma=1.0):\n",
    "    if len(liste_observations) < taille_mini_lot:\n",
    "        return\n",
    "    observations = liste_observations.mini_lot_observations(taille_mini_lot)\n",
    "    # Transposer le mini_lot\n",
    "    mini_lot = Observation(*zip(*observations))\n",
    "\n",
    "    # masque_non_final[i] est True si etat[i] n'est pas final\n",
    "    masque_non_final = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          mini_lot.etat_suivant)),dtype=torch.bool)\n",
    "    non_final_etat_suivants = torch.cat([s for s in mini_lot.etat_suivant if s is not None])\n",
    "    mini_lot_etats = torch.cat(mini_lot.etat)\n",
    "    mini_lot_actions = torch.cat(mini_lot.action)\n",
    "    mini_lot_recompenses = torch.cat(mini_lot.recompense)\n",
    "\n",
    "    # Chercher la valeur de Q(s,a) pour l'action a qui a été choisie selon l'historique\n",
    "    mini_lot_Q = rnaq(mini_lot_etats).gather(1, mini_lot_actions)\n",
    "\n",
    "    # Calculer la valeur cible (R+maxQ) selon la politique cible (Q est 0 si état final)\n",
    "    mini_lot_Q_suivant = torch.zeros(taille_mini_lot)\n",
    "    mini_lot_Q_suivant[masque_non_final] = rnaq(non_final_etat_suivants).max(1)[0].detach()\n",
    "    mini_lot_Q_cibles = (mini_lot_Q_suivant * gamma) + mini_lot_recompenses\n",
    "\n",
    "    # Calculer l'erreur Huber\n",
    "    #cout = F.smooth_l1_loss(mini_lot_Q, mini_lot_Q_cibles.unsqueeze(1))\n",
    "    cout = F.mse_loss(mini_lot_Q, mini_lot_Q_cibles.unsqueeze(1))\n",
    "\n",
    "    # Retropropagation\n",
    "    optimiseur.zero_grad()\n",
    "    cout.backward()\n",
    "    for parametre in rnaq.parameters(): # Limiter les gradients (clip) pour stabiliser l'optimisation\n",
    "        parametre.grad.data.clamp_(-1, 1)\n",
    "    optimiseur.step()\n",
    "    \n",
    "def optimiser_DQN(env, nombre_episodes=20,  gamma=1.0, alpha=0.1, epsilon_max=1, epsilon_min=0.05,\n",
    "                  epsilon_taux_decroissance = 0.005, frequence_maj_rnaq_cible=10):    \n",
    "    longueur_episode = np.zeros(nombre_episodes)\n",
    "    for i_episode in range(nombre_episodes):\n",
    "        env.reset() # Initialiser l'environnement et l'état\n",
    "        ecran_precedent = chercher_ecran()\n",
    "        ecran_actuel = chercher_ecran()\n",
    "        etat = ecran_actuel - ecran_precedent # Différence entre deux images d'écran consécutives\n",
    "        # Calculer epsilon\n",
    "        epsilon = epsilon_min + (epsilon_max - epsilon_min)*np.exp(-epsilon_taux_decroissance*i_episode) \n",
    "\n",
    "        compteur_action = 0\n",
    " \n",
    "        for t in count():\n",
    "            compteur_action +=1\n",
    "            action = choisir_action(etat,epsilon) # Sélectionner et exécuter l'action\n",
    "            _, recompense, fin_episode, _ = env.step(action.item())\n",
    "            recompense = torch.tensor([recompense])\n",
    "            # Produire l'état suivant\n",
    "            ecran_precedent = ecran_actuel\n",
    "            ecran_actuel = chercher_ecran()\n",
    "            if not fin_episode:\n",
    "                etat_suivant = ecran_actuel - ecran_precedent\n",
    "            else:\n",
    "                etat_suivant = None\n",
    "            # Ajouter l'observation à l'historique\n",
    "            liste_observations.ajouter_historique(etat, action, etat_suivant, recompense)\n",
    "            etat = etat_suivant\n",
    "            \n",
    "            optimisation_RNAQ() # Optimiser le RNAQ avec un mini-lot\n",
    "            if fin_episode:\n",
    "                longueur_episode[i_episode] = t\n",
    "                break\n",
    "            \n",
    "        print(\"\\rEpisode {}/{}. Longueur:{}\".format(i_episode+1, nombre_episodes, t), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return longueur_episode\n",
    "\n",
    "longueur_episode = optimiser_DQN(env, nombre_episodes=200,gamma=0.999,alpha=0.1,epsilon_max=1,epsilon_min=0.05,\n",
    "                  epsilon_taux_decroissance = 0.01,frequence_maj_rnaq_cible=10)\n",
    "\n",
    "env.close()\n",
    "\n",
    "afficher_longueur_episode(longueur_episode,fenetre=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
