{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "# Implémentation d'un RNA par couche\n",
    "# Ajout de CoucheConvolution calculs détaillés\n",
    "# Exemple avec MNIST\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42) # pour reproduire les mêmes résultats\n",
    "import random\n",
    "random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "class Couche:\n",
    "    \"\"\" Classe abstraite qui représente une couche du RNA\n",
    "        X:  np.array, entrée de la couche \n",
    "        Y: np.array, sortie de la couche\n",
    "    \"\"\"\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Calculer la sortie Y pour une valeur de X\n",
    "        \n",
    "        X : vecteur des variables prédictives\n",
    "        Les valeurs de X et Y sont stockées pour les autres traitements.\n",
    "        \"\"\"\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées par rapport à X et les autres paramètres à partir de dJ_dY\n",
    "        et mettre à jour les paramètres de la couche selon le taux spécifié.\n",
    "        \n",
    "        dJ_dY : np.array(1,m), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "\n",
    "class CoucheDenseLineaire(Couche):\n",
    "    \"\"\" Couche linéaire dense. Y=WX+B\n",
    "    \"\"\"\n",
    "    def __init__(self,n,m,init_W=None,init_B=None):\n",
    "        \"\"\" Initilalise les paramètres de la couche. W et B sont initialisés avec init_W et init_B lorsque spécifiés.\n",
    "        Sinon, des valeurs aléatoires sont générés pour W une distribution normale et B est initialisée avec des 0 \n",
    "        si les paramètres init_W et init_B ne sont pas spécifiés.\n",
    "        L'initialization He est employée pour W\n",
    "        n : int, taille du vecteur d'entrée X\n",
    "        m : int, taille du vecteur de sortie Y\n",
    "        init_W : np.array, shape(n,m), valeur initiale optionnelle de W\n",
    "        init_B : np.array, shape(1,m), valeur initial optionnelle de B\n",
    "        \"\"\"\n",
    "        if init_W is None :\n",
    "            # Initialization He\n",
    "            self.W = np.random.randn(n,m) * np.sqrt(2/n) \n",
    "        else:\n",
    "            self.W = init_W\n",
    "        if init_B is None :\n",
    "            self.B = np.zeros((1,m))\n",
    "        else:\n",
    "            self.B = init_B\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Fait la propagation de X et retourne Y=WX+B. \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.B + np.dot(self.X,self.W)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Calculer les dérivées dJ_dW,dJ_dB,dJ_dX pour une couche dense linéaire et\n",
    "        mettre à jour les paramètres selon le taux spécifié\n",
    "        \n",
    "        dJ_dY : np.array(1,2), dérivées de J par rapport à la sortie Y\n",
    "        taux : float, le taux dans la descente de gradiant\n",
    "        retourne la dérivée de J par rapport à X\n",
    "        \"\"\"\n",
    "        dJ_dW = np.dot(self.X.T,dJ_dY)\n",
    "        dJ_dB = dJ_dY\n",
    "        dJ_dX = np.dot(dJ_dY,self.W.T)\n",
    "        if trace:\n",
    "            print(\"dJ_dW:\",dJ_dW)\n",
    "            print(\"dJ_dB:\",dJ_dB)\n",
    "            print(\"dJ_dX:\",dJ_dX)\n",
    "        # Metre à jour les paramètres W et B\n",
    "        self.W -= taux * dJ_dW\n",
    "        self.B -= taux * dJ_dB\n",
    "        if trace:\n",
    "            print(\"W modifié:\",self.W)\n",
    "            print(\"B modifié:\",self.B)\n",
    "        return dJ_dX\n",
    "\n",
    "    \n",
    "class CoucheActivation(Couche):\n",
    "    \"\"\" Couche d'activation selon une fonction spécifiée dans le constructeur\n",
    "    \"\"\"\n",
    "    def __init__(self,fonction_activation,derivee):\n",
    "        \"\"\" Initialise la fonction_activation ainsi que la dérivée\n",
    "        fonction_activation: une fonction qui prend chacune des valeurs de X et \n",
    "        retourne Y=fonction_activation(X)\n",
    "        derivee: une fonction qui calcule la dérivée la fonction_activation\n",
    "        \"\"\"\n",
    "        self.fonction_activation = fonction_activation\n",
    "        self.derivee = derivee\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Retourne Y=fonction_activation(X)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = self.fonction_activation(self.X)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Retourne la dérivée de la fonction d'activation par rapport l'entrée X\n",
    "        Le taux n'est pas utilisé parce qu'il n'y a pas de paramètres à modifier dans ce genre de couche\n",
    "        \"\"\"\n",
    "        return self.derivee(self.X) * dJ_dY\n",
    "\n",
    "class CoucheSoftmax(Couche):\n",
    "    \"\"\" Couche d'activation softmax\n",
    "    \"\"\"\n",
    "    def __init__(self,n):\n",
    "        \"\"\"\n",
    "        n: nombre d'entrées et de sorties\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def propager_une_couche(self,X):\n",
    "        \"\"\" Calcule les activations softmax pour chacunes de entrées xi\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        X_decale = X-np.max(X) # Pour la stabilité numérique, les valeurs sont décalées de max(X)\n",
    "        exponentielles = np.exp(X_decale)\n",
    "        self.Y = exponentielles / np.sum(exponentielles)\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self,dJ_dY,taux,trace=False):\n",
    "        \"\"\" Retourne la dérivée de la fonction d'activation par rapport l'entrée X\n",
    "        Le taux n'est pas utilisé parce qu'il n'y a pas de paramètres à modifier dans ce genre de couche\n",
    "        \"\"\"\n",
    "        return np.dot(dJ_dY,self.Y.T*(np.identity(self.n)-self.Y))\n",
    "\n",
    "class CoucheConvolution(Couche):\n",
    "    \"\"\" Couche convolution qui calcule en réalité la corrélation avec les filtres F \"\"\"\n",
    "\n",
    "    def __init__(self,forme_X , forme_filtre, profondeur_Y):\n",
    "        \"\"\" \n",
    "        forme_X = (largeur X, hauteur X, profondeur X)\n",
    "        forme_filtre = (largeur filtre, hauteur filtre)\n",
    "        profondeur_Y = profondeur de Y (nombre de filtres de la couche)\"\"\"\n",
    "        self.forme_X = forme_X\n",
    "        self.profondeur_X = forme_X[2]\n",
    "        self.forme_filtre = forme_filtre\n",
    "        self.profondeur_Y = profondeur_Y\n",
    "        self.forme_Y = (forme_X[0]-forme_filtre[0]+1, forme_X[1]-forme_filtre[1]+1, profondeur_Y)\n",
    "        self.F = np.random.rand(forme_filtre[0], forme_filtre[1], self.profondeur_X, profondeur_Y) - 0.5\n",
    "        self.B = np.random.rand(profondeur_Y) - 0.5\n",
    "\n",
    "    def propager_une_couche(self, X):\n",
    "        self.X = X\n",
    "        self.Y = np.zeros(self.forme_Y)\n",
    "\n",
    "        for indice_profondeur_Y in range(self.profondeur_Y):\n",
    "            for indice_profondeur_X in range(self.profondeur_X):\n",
    "                for i in range(self.forme_X[0]-self.forme_filtre[0]+1):\n",
    "                    for j in range(self.forme_X[1]-self.forme_filtre[1]+1):\n",
    "                        correlation2d = np.sum(self.X[i:i+self.forme_filtre[0],j:j+self.forme_filtre[1],indice_profondeur_X]*\n",
    "                                               self.F[:,:,indice_profondeur_X,indice_profondeur_Y])+self.B[indice_profondeur_Y]\n",
    "                        self.Y[i,j,indice_profondeur_Y] += correlation2d\n",
    "        return self.Y\n",
    "    \n",
    "    def retropropager_une_couche(self, dJ_dY, taux, trace=False):\n",
    "        \"\"\"Calculer les gradiants pour F, B et X de la couche par rapport à J \"\"\"\n",
    "        dJ_dX = np.zeros(self.forme_X)\n",
    "        dJ_dF = np.zeros((self.forme_filtre[0], self.forme_filtre[1], self.profondeur_X, self.profondeur_Y))\n",
    "        dB = np.zeros(self.profondeur_Y)\n",
    "\n",
    "        for indice_profondeur_Y in range(self.profondeur_Y):\n",
    "            for indice_profondeur_X in range(self.profondeur_X):\n",
    "                for i in range(self.forme_X[0]):\n",
    "                     for j in range(self.forme_X[1]):\n",
    "                         convolution2d = 0\n",
    "                         for r in range(max(0,i-self.forme_filtre[0]+1),min(self.forme_X[0]-self.forme_filtre[0],i)):\n",
    "                             for s in range(max(0,j-self.forme_filtre[1]+1),min(self.forme_X[1]-self.forme_filtre[1],j)):\n",
    "                                 convolution2d += dJ_dY[r,s,indice_profondeur_Y]*self.F[i-r,j-s,indice_profondeur_X,indice_profondeur_Y]\n",
    "                         dJ_dX[i,j,indice_profondeur_X] += convolution2d\n",
    "\n",
    "                for i in range(self.forme_filtre[0]):\n",
    "                     for j in range(self.forme_filtre[1]):\n",
    "                         dJ_dF[i,j,indice_profondeur_X,indice_profondeur_Y] = np.sum(self.X[i:i+self.forme_X[0]-\n",
    "                                self.forme_filtre[0]+1,j:j+self.forme_X[1]-self.forme_filtre[1]+1,indice_profondeur_X]*dJ_dY[:,:,indice_profondeur_Y])\n",
    "                     dB[indice_profondeur_Y] = np.sum(dJ_dY[:,:,indice_profondeur_Y])\n",
    "        self.F -= taux*dJ_dF\n",
    "        self.B -= taux*dB\n",
    "        return dJ_dX    \n",
    "    \n",
    "class CoucheApplatissement(Couche):\n",
    "    \"\"\"Produire une forme applatie de l'entrée X\"\"\"\n",
    "    def propager_une_couche(self, X):\n",
    "        self.X = X\n",
    "        self.Y = X.reshape((1,-1))\n",
    "        return self.Y\n",
    "\n",
    "    def retropropager_une_couche(self, dJ_dY, taux, trace=False):\n",
    "        return dJ_dY.reshape(self.X.shape)\n",
    "\n",
    "\n",
    "def erreur_quadratique(y_prediction,y):\n",
    "    \"\"\" Retourne l'erreur quadratique entre la prédiction y_prediction et la valeur attendue y\n",
    "    \"\"\" \n",
    "    return np.sum(np.power(y_prediction-y,2))\n",
    "\n",
    "def d_erreur_quadratique(y_prediction,y):\n",
    "    return 2*(y_prediction-y)\n",
    "\n",
    "def entropie_croisee(y_prediction,y):\n",
    "    \"\"\" Retourne l'entropie croisée entre la prédiction y_prediction et la valeur attendue y\n",
    "    \"\"\" \n",
    "    return -np.sum(y*np.log(y_prediction))\n",
    "\n",
    "def d_entropie_croisee(y_prediction,y):\n",
    "    return -(y/y_prediction)\n",
    "\n",
    "class ReseauMultiCouches:\n",
    "    \"\"\" Réseau mutli-couche formé par une séquence de Couches\n",
    "    \n",
    "    couches : liste de Couches du RNA\n",
    "    cout : fonction qui calcule de cout J\n",
    "    derivee_cout: dérivée de la fonction de cout\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.couches = []\n",
    "        self.cout = None\n",
    "        self.derivee_cout = None\n",
    "\n",
    "    def ajouter_couche(self,couche):\n",
    "        self.couches.append(couche)\n",
    "\n",
    "    def specifier_J(self,cout,derivee_cout):\n",
    "        \"\"\" Spécifier la fonction de coût J et sa dérivée\n",
    "        \"\"\"\n",
    "        self.cout = cout\n",
    "        self.derivee_cout = derivee_cout\n",
    "\n",
    "    def propagation_donnees_X(self,donnees_X,trace=False):\n",
    "        \"\"\" Prédire Y pour chacune des observations dans donnees_X)\n",
    "        donnees_X : np.array 3D des valeurs de X pour chacune des observations\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_X)\n",
    "        predictions_Y = []\n",
    "        for indice_observation in range(nb_observations):\n",
    "            # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "            # à la valeur de Y de la couche précédente\n",
    "            XY_propage = donnees_X[indice_observation]\n",
    "            if trace: \n",
    "                print(\"Valeur de X initiale:\",XY_propage)\n",
    "            for couche in self.couches:\n",
    "                XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                if trace: \n",
    "                    print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "            predictions_Y.append(XY_propage)\n",
    "\n",
    "        return predictions_Y\n",
    "\n",
    "    def metriques(self, donnees_X,donnees_Y):\n",
    "        \"\"\"Retourne le cout moyen, la proportion de bons résultats\n",
    "        Choisit l'indice de la classe dont l'activation est la plus grande\"\"\"\n",
    "        erreur_quadratique = 0\n",
    "        nb_correct = 0\n",
    "        predictions_Y=self.propagation_donnees_X(donnees_X)\n",
    "        for indice in range(len(donnees_Y)):\n",
    "            erreur_quadratique += self.cout(predictions_Y[indice],donnees_Y[indice])\n",
    "            classe_predite = np.argmax(predictions_Y[indice])\n",
    "            if donnees_Y[indice][0,classe_predite] == 1:\n",
    "                nb_correct+=1\n",
    "        return (erreur_quadratique/len(donnees_Y),nb_correct/len(donnees_Y))\n",
    "\n",
    "    def entrainer_descente_gradiant_stochastique(self,donnees_ent_X,donnees_ent_Y,donnees_test_X,donnees_test_Y,\n",
    "                                                 nb_epochs,taux,trace=False,graph_cout=False):\n",
    "        \"\"\" Entrainer le réseau par descente de gradiant stochastique (une observation à la fois)\n",
    "        \n",
    "        donnees_ent_X : np.array 3D des valeurs de X pour chacune des observations d'entrainement\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_ent_Y : np.array 3D des valeurs de Y pour chacune des observations d'entrainement\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        donnees_test_X : np.array 3D des valeurs de X pour chacune des observations de test\n",
    "            chacun des X est un np.array 2D de taille (1,n)\n",
    "        donnees_test_Y : np.array 3D des valeurs de Y pour chacune des observations de test\n",
    "            chacun des Y est un np.array 2D de taille (1,m)\n",
    "        nb_epochs : nombre de cycle de passage sur les données d'entainement\n",
    "        taux : taux dans la descente de gradiant\n",
    "        trace : Boolean, True pour afficher une trace des calculs effectués sur les paramètres\n",
    "        graph_cout : Boolean, True pur afficher un graphique de l'évolution du coût\n",
    "        \"\"\"\n",
    "        nb_observations = len(donnees_ent_X)\n",
    "        if graph_cout :\n",
    "            liste_cout_moyen_ent = []\n",
    "            liste_ok_ent = []\n",
    "            liste_cout_moyen_test = []\n",
    "            liste_ok_test = []\n",
    "\n",
    "        # Boucle d'entrainement principale, nb_epochs fois\n",
    "        for cycle in range(nb_epochs):\n",
    "            cout_total = 0\n",
    "            # Descente de gradiant stochastique, une observation à la fois\n",
    "            for indice_observation in range(nb_observations):\n",
    "                # Propagation avant pour une observation X\n",
    "                # XY_propage : contient la valeur de X de la couche courante qui correspond \n",
    "                # à la valeur de Y de la couche précédente\n",
    "                XY_propage = donnees_ent_X[indice_observation]\n",
    "                if trace: \n",
    "                    print(\"Valeur de X initiale:\",XY_propage)\n",
    "\n",
    "                for couche in self.couches:\n",
    "                    XY_propage = couche.propager_une_couche(XY_propage)\n",
    "                    if trace: \n",
    "                        print(\"Valeur de Y après propagation pour la couche:\",XY_propage)\n",
    "\n",
    "                # Calcul du coût pour une observation\n",
    "                cout_total += self.cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "\n",
    "                # Rétropropagation pour une observation\n",
    "                # dJ_dX_dJ_dY représente la valeur de la dérivée dJ_dX de la couche suivante\n",
    "                # qui correspond à dJ_dY de la couche en cours de traitement\n",
    "                dJ_dX_dJ_dY = self.derivee_cout(XY_propage,donnees_ent_Y[indice_observation])\n",
    "                if trace :\n",
    "                    print(\"dJ_dY pour la couche finale:\",dJ_dX_dJ_dY)\n",
    "                for couche in reversed(self.couches):\n",
    "                    dJ_dX_dJ_dY = couche.retropropager_une_couche(dJ_dX_dJ_dY,taux,trace)\n",
    "\n",
    "            # Calculer et afficher le coût moyen pour une epoch\n",
    "            cout_moyen = cout_total/nb_observations\n",
    "            if graph_cout:\n",
    "                print(f'-------- > epoch {cycle+1}:  coût moyen {cout_moyen}')\n",
    "                cout_ent,ok_ent = self.metriques(donnees_ent_X,donnees_ent_Y)\n",
    "                cout_test,ok_test = self.metriques(donnees_test_X,donnees_test_Y)\n",
    "                liste_cout_moyen_ent.append(cout_ent)\n",
    "                liste_ok_ent.append(ok_ent)\n",
    "                liste_cout_moyen_test.append(cout_test)\n",
    "                liste_ok_test.append(ok_test)\n",
    "                \n",
    "            \n",
    "        # Affichage du graphique d'évolution de l'erreur quadratique\n",
    "        if graph_cout:\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_ent,label='Erreur entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_cout_moyen_test,label='Erreur test')\n",
    "            plt.title(\"Evolution du coût\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_ent,label='entraînement')\n",
    "            plt.plot(np.arange(0,nb_epochs),liste_ok_test,label='test')\n",
    "            plt.title(\"Evolution du taux de bonnes prédictions\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('moyenne par observation')\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.show()\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivee_tanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoide(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def derivee_sigmoide(x):\n",
    "    return sigmoide(x)*(1-sigmoide(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def derivee_relu(x):\n",
    "    return np.heaviside(x,1)\n",
    "\n",
    "def bitmap(classe):\n",
    "    \"\"\" Representer l'entier de classe par un vecteur bitmap (10,1) \n",
    "    classe : entier (entre 0 et 9 qui représente la classe de l'observation\"\"\"\n",
    "    e = np.zeros((1,10))\n",
    "    e[0,classe] = 1.0\n",
    "    return e\n",
    "\n",
    "# Chargement des données de MNIST\n",
    "import pickle, gzip\n",
    "\n",
    "fichier_donnees = gzip.open(r\"mnist.pkl.gz\", 'rb')\n",
    "donnees_ent, donnees_validation, donnees_test = pickle.load(fichier_donnees, encoding='latin1')\n",
    "fichier_donnees.close()\n",
    "    \n",
    "donnees_ent_X = donnees_ent[0].reshape((50000,28,28,1))\n",
    "donnees_ent_Y = [bitmap(y) for y in donnees_ent[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "donnees_test_X = donnees_test[0].reshape((10000,28,28,1))\n",
    "donnees_test_Y = [bitmap(y) for y in donnees_test[1]] # Encodgae bitmap de l'entier (one hot encoding)\n",
    "\n",
    "# Définir l'architecture du RNA \n",
    "un_RNA = ReseauMultiCouches()\n",
    "un_RNA.specifier_J(entropie_croisee,d_entropie_croisee)\n",
    "un_RNA.ajouter_couche(CoucheConvolution((28,28,1),(3,3),5))\n",
    "un_RNA.ajouter_couche(CoucheActivation(relu,derivee_relu))\n",
    "un_RNA.ajouter_couche(CoucheApplatissement())\n",
    "un_RNA.ajouter_couche(CoucheDenseLineaire(26*26*5,10))\n",
    "un_RNA.ajouter_couche(CoucheSoftmax(10))\n",
    "\n",
    "# Entrainer le RNA\n",
    "un_RNA.entrainer_descente_gradiant_stochastique(donnees_ent_X[:5000],donnees_ent_Y[:5000],donnees_test_X[:1000],donnees_test_Y[:1000],\n",
    "                                                nb_epochs=10,taux=0.01,trace = False, graph_cout = True)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Classe de l'image\",i,\":\",donnees_ent_Y[i])\n",
    "    print(\"Prédiction softmax:\",un_RNA.propagation_donnees_X(donnees_ent_X[i:i+1]))\n",
    "    plt.imshow(donnees_ent_X[i].reshape(28,28), cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
