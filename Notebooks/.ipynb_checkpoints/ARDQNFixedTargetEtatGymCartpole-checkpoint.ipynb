{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Deep Q-learning avec cibles-Q fixes\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "random.seed(42)\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "Observation = namedtuple('Observation',\n",
    "                        ('etat', 'action', 'etat_suivant', 'recompense'))\n",
    "class HistoriqueObservations(object):\n",
    "    \"\"\" Stockage de la liste des observations pour l'apprentissage \n",
    "        Mémoire circulaire pour limiter à taille_memoire\"\"\"\n",
    "    def __init__(self, taille_memoire):\n",
    "        self.taille_memoire = taille_memoire\n",
    "        self.indice_courant = 0\n",
    "        self.liste_observations = []\n",
    "\n",
    "\n",
    "    def ajouter_historique(self, *args):\n",
    "        \"\"\"AJoutee une observation. Écriture circulaire.\"\"\"\n",
    "        if len(self.liste_observations) < self.taille_memoire:\n",
    "            self.liste_observations.append(None)\n",
    "        self.liste_observations[self.indice_courant] = Observation(*args)\n",
    "        self.indice_courant = (self.indice_courant + 1) % self.taille_memoire\n",
    "\n",
    "    def mini_lot_observations(self, taille_mini_lot):\n",
    "        return random.sample(self.liste_observations, taille_mini_lot)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.liste_observations)\n",
    "    \n",
    "\n",
    "        \n",
    "class RNAQ(nn.Module):\n",
    "    def __init__(self,taille_couche_cachee=164,nb_actions_y=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, taille_couche_cachee)\n",
    "        self.l2 = nn.Linear(taille_couche_cachee, nb_actions_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "# Initaliser le RNAQ et RNACible\n",
    "n_actions = env.action_space.n\n",
    "rnaq = RNAQ(taille_couche_cachee=164,nb_actions_y=2)\n",
    "rnaq_cible = RNAQ(taille_couche_cachee=164,nb_actions_y=2)\n",
    "rnaq_cible.load_state_dict(rnaq.state_dict())\n",
    "rnaq_cible.eval()\n",
    "\n",
    "optimiseur = optim.Adam(rnaq.parameters(), 0.001)\n",
    "liste_observations = HistoriqueObservations(10000)\n",
    "\n",
    "nombre_etapes_accomplies = 0\n",
    "\n",
    "def choisir_action(etat,epsilon):\n",
    "    \"\"\" Choisir l'action max Q (politique e-vorace) à partir de rnaq\"\"\"\n",
    "    unif_01 = random.uniform(0, 1)\n",
    "    if unif_01 > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return rnaq(etat).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]],dtype=torch.long)\n",
    "\n",
    "durees_par_episode = []\n",
    "\n",
    "def afficher_longueur_episode(longueur_episode, fenetre=10):\n",
    "    \"\"\"\n",
    "    Afficher l'évolution des longueurs d'épisodes avec le temps\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(longueur_episode,label=\"Longueur épisode\")\n",
    "    longueur_moyenne_fenetre=[longueur_episode[i:i+fenetre].mean() for i in range(longueur_episode.shape[0]-fenetre)]\n",
    "    plt.plot(np.arange(fenetre,longueur_episode.shape[0]),longueur_moyenne_fenetre,label=\"Moyenne mobile\")\n",
    "    plt.xlabel(\"Épisode\")\n",
    "    plt.ylabel(\"Longueur épisode\")\n",
    "    plt.title(\"DQN (Deep Q-learning), Cartpole : évolution de la longueur d'épisode, fenetre:\"+str(fenetre))\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def optimisation_RNAQ(taille_mini_lot = 16, gamma=1.0):\n",
    "    if len(liste_observations) < taille_mini_lot:\n",
    "        return\n",
    "    observations = liste_observations.mini_lot_observations(taille_mini_lot)\n",
    "    # Transposer le mini_lot\n",
    "    mini_lot = Observation(*zip(*observations))\n",
    "\n",
    "    # masque_non_final[i] est True si etat[i] n'est pas final\n",
    "    masque_non_final = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          mini_lot.etat_suivant)),dtype=torch.bool)\n",
    "    non_final_etat_suivants = torch.cat([s for s in mini_lot.etat_suivant if s is not None])\n",
    "    mini_lot_etats = torch.cat(mini_lot.etat)\n",
    "    mini_lot_actions = torch.cat(mini_lot.action)\n",
    "    mini_lot_recompenses = torch.cat(mini_lot.recompense)\n",
    "\n",
    "    # Chercher la valeur de Q(s,a) pour l'action a qui a été choisie selon l'historique\n",
    "    mini_lot_Q = rnaq(mini_lot_etats).gather(1, mini_lot_actions)\n",
    "\n",
    "    # Calculer la valeur cible (R+maxQ) selon la politique cible (Q est 0 si état final)\n",
    "    mini_lot_Q_suivant = torch.zeros(taille_mini_lot)\n",
    "    mini_lot_Q_suivant[masque_non_final] = rnaq_cible(non_final_etat_suivants).max(1)[0].detach()\n",
    "    mini_lot_Q_cibles = (mini_lot_Q_suivant * gamma) + mini_lot_recompenses\n",
    "\n",
    "    # Calculer l'erreur Huber\n",
    "    #cout = F.smooth_l1_loss(mini_lot_Q, mini_lot_Q_cibles.unsqueeze(1))\n",
    "    cout = F.mse_loss(mini_lot_Q, mini_lot_Q_cibles.unsqueeze(1))\n",
    "\n",
    "    # Retropropagation\n",
    "    optimiseur.zero_grad()\n",
    "    cout.backward()\n",
    "    for parametre in rnaq.parameters(): # Limiter les gradients (clip) pour stabiliser l'optimisation\n",
    "        parametre.grad.data.clamp_(-1, 1)\n",
    "    optimiseur.step()\n",
    "    \n",
    "def optimiser_DQN(env, nombre_episodes=20,  gamma=1.0, alpha=0.1, epsilon_max=1, epsilon_min=0.05,\n",
    "                  epsilon_taux_decroissance = 0.005):    \n",
    "    longueur_episode = np.zeros(nombre_episodes)\n",
    "    for i_episode in range(nombre_episodes):\n",
    "        etat=env.reset() # Initialiser l'environnement et l'état\n",
    "        etat = torch.FloatTensor([etat])\n",
    "        # Calculer epsilon\n",
    "        epsilon = epsilon_min + (epsilon_max - epsilon_min)*np.exp(-epsilon_taux_decroissance*i_episode) \n",
    "\n",
    "        compteur_action = 0\n",
    " \n",
    "        for t in count():\n",
    "            #env.render()\n",
    "            compteur_action +=1\n",
    "            action = choisir_action(etat,epsilon) # Sélectionner et exécuter l'action\n",
    "            etat_suivant, recompense, fin_episode, _ = env.step(action.item())\n",
    "            recompense = torch.tensor([recompense])\n",
    "            etat_suivant = torch.FloatTensor([etat_suivant])\n",
    "            if fin_episode:\n",
    "                etat_suivant = None\n",
    "\n",
    "            # Ajouter l'observation à l'historique\n",
    "            liste_observations.ajouter_historique(etat, action,etat_suivant,recompense)\n",
    "            etat = etat_suivant\n",
    "            \n",
    "            optimisation_RNAQ() # Optimiser le RNAQ avec un mini-lot\n",
    "            if fin_episode:\n",
    "                longueur_episode[i_episode] = t\n",
    "                break\n",
    "        # Mettre à jouer le RNAQcible selon FREQUENCE_MAJ_RNAQ_CIBLE \n",
    "        if i_episode % 10 == 0:\n",
    "            rnaq_cible.load_state_dict(rnaq.state_dict())\n",
    "            \n",
    "        print(\"\\rEpisode {}/{}. Longueur:{}\".format(i_episode+1, nombre_episodes, t))\n",
    "        #sys.stdout.flush()\n",
    "\n",
    "    return longueur_episode\n",
    "\n",
    "longueur_episode = optimiser_DQN(env, nombre_episodes=200,gamma=1,alpha=0.1,epsilon_max=1,epsilon_min=0.05,\n",
    "                  epsilon_taux_decroissance = 0.01)\n",
    "\n",
    "env.close()\n",
    "\n",
    "afficher_longueur_episode(longueur_episode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
