{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Exemple de réseau de neuronne à propagation avant et\n",
    "rétropropagation de l'erreur pour l'apprentissage\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42) # pour reproduire les mêmes résultats\n",
    "random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoide(z):\n",
    "    \"\"\"The sigmoide function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def derivee_sigmoide(z):\n",
    "    \"\"\"Derivative of the sigmoide function.\"\"\"\n",
    "    return sigmoide(z)*(1-sigmoide(z))\n",
    "\n",
    "def bitmap(classe):\n",
    "    \"\"\" Representer l'entier de classe par un vecteur bitmap (10,1) \n",
    "    classe : entier entre 0 et 9 qui représente la classe de l'observation\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[classe] = 1.0\n",
    "    return e\n",
    "\n",
    "class RNA(object):\n",
    "    \"\"\" Un RNA est un réseau de neuronnes artificiel multi-couche.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ncs):\n",
    "        \"\"\" ncs[c] contient le nombre de neurones de la couche c, c = 0 ...nombre_couches-1\n",
    "        la couche d'indice 0 est la couche d'entrée\n",
    "        ncs[nombre_couches-1] doit correspondre au nombre de catégories des y (sortie)\n",
    "        \n",
    "        liste_w[c] est la matrice des poids entre la couche c et c+1\n",
    "        liste_w[c][i,j] est le poids entre le neuronne i de la couche c+1 et j de la couche c\n",
    "        i = 0 correspond au biais par convention\n",
    "        les poids sont initialisés avec un nombre aléatoire selon une distribution N(0,1)\n",
    "        \"\"\"\n",
    "        self.nombre_couches = len(ncs)\n",
    "        self.ncs = ncs\n",
    "        self.liste_w = [np.random.randn(x+1,y) for x, y in zip(ncs[:-1], ncs[1:])]\n",
    "\n",
    "    def propagation_avant_w(self, activation):\n",
    "        \"\"\"\n",
    "        Traiter une entrée par propagation avant\n",
    "        \n",
    "        activation: activation initiale qui correspond aux entrées (taille self.ncs[0])\n",
    "        retourne l'activation de sortie après propagation avant\"\"\"\n",
    "        \n",
    "        for w in self.liste_w:\n",
    "            activation = np.vstack((np.ones(1),sigmoide(np.dot(w.transpose(),activation))))\n",
    "        return activation\n",
    "\n",
    "    def entrainer_par_mini_lot(self,donnees_entrainement,donnees_test,nombre_epochs,taille_mini_lot,eta):\n",
    "        \"\"\"\n",
    "        Entrainer le RNA par mini-lots\n",
    "        Affiche le nombre de bons résultats des donnees_test pour chaque epoch\n",
    "        \n",
    "        donnees_entrainement : liste de tuples (x,y) pour l'entrainement où\n",
    "            x est un tableau de taille (ncs[0],1) où n est la taille des entrées\n",
    "            y est un encodage bitmap de la catégorie en tableau de taille ncs[nombre_couches-1]\n",
    "        donnees_test : liste de tuples (x,y) pour les tests\n",
    "            x est un tableau de taille (ncs[0],1) où n est la taille des entrées\n",
    "            y un int où 0<=y< nombre de catégories\n",
    "        nombre_epochs : nombre de passe d'entrainement\n",
    "        taille_mini_lot : la taille de chacun des mini-lots\n",
    "        eta : vitesse d'apprentissage\n",
    "        \"\"\"\n",
    "        n_test = len(donnees_test)\n",
    "        n_ent = len(donnees_entrainement)\n",
    "        self.liste_eqm_ent = []\n",
    "        self.liste_abs_ent = []\n",
    "        self.liste_eqm_test = []\n",
    "        self.liste_abs_test = []\n",
    "        \n",
    "        for j in range(nombre_epochs):\n",
    "            random.shuffle(donnees_entrainement)\n",
    "            mini_lots = [donnees_entrainement[k:k+taille_mini_lot] for k in range(0, n_ent, taille_mini_lot)]\n",
    "            # Entrainer un mimi-lot à la fois\n",
    "            for mini_lot in mini_lots:\n",
    "                # Initialiser les gradiants totaux à 0\n",
    "                liste_dJ_dw = [np.zeros(w.shape) for w in self.liste_w]\n",
    "                for x, y in mini_lot:\n",
    "                    dJ_dw_une_ligne = self.retropropagation_w(x, y)\n",
    "                    # ajouter les gradiants d'une observation aux totaux partiels du lot\n",
    "                    liste_dJ_dw = [dJ_dw+dJ_dw_1 for (dJ_dw, dJ_dw_1) in zip(liste_dJ_dw, dJ_dw_une_ligne)]\n",
    "                # mettre à jour les paramètres du RNA avec les gradiants du lot    \n",
    "                self.liste_w = [w-(eta/len(mini_lot))*dw  for (w, dw) in zip(self.liste_w, liste_dJ_dw)]\n",
    "            \n",
    "             # Calcul des métriques de performance\n",
    "            eqm_ent,abs_ent = self.metriques(donnees_entrainement)\n",
    "            eqm_test,abs_test = self.metriques(donnees_test)\n",
    "            self.liste_eqm_ent.append(eqm_ent/n_ent)\n",
    "            self.liste_abs_ent.append(abs_ent/n_ent)\n",
    "            self.liste_eqm_test.append(eqm_test/n_test)\n",
    "            self.liste_abs_test.append(abs_test/n_test)\n",
    "            print(\"Epoch {0}: EQM entrainement: {1} Erreur absolue: {2}\".format(j, eqm_ent/n_ent,abs_ent/n_ent))\n",
    "            print(\"Epoch {0}: EQM test: {1} Erreur absolue: {2}\".format(j, eqm_test/n_test,abs_test/n_test))\n",
    "\n",
    "        # Affichage des graphiques d'évolution des performances par epoch\n",
    "        plt.plot(np.arange(0,nombre_epochs),self.liste_eqm_ent,label='Eqm entraînement')\n",
    "        plt.plot(np.arange(0,nombre_epochs),self.liste_eqm_test,label='Eqm test')\n",
    "        plt.title(\"Erreur quadratique moyenne\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('erreur')\n",
    "        plt.legend(loc='upper center')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(0,nombre_epochs),self.liste_abs_ent,label='Erreur absolue entraînement')\n",
    "        plt.plot(np.arange(0,nombre_epochs),self.liste_abs_test,label='Erreur absolue test')\n",
    "        plt.title(\"Erreur absolue\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('nb correct')\n",
    "        plt.legend(loc='upper center')\n",
    "        plt.show()\n",
    "    \n",
    "    def retropropagation_w(self, x, y):\n",
    "        \"\"\"Return a tuple ``(dJ_db, dJ_dw)`` representing the\n",
    "        gradient for the cost function C_x.  ``dJ_db`` and\n",
    "        ``dJ_dw`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.liste_biais`` and ``self.liste_w``.\"\"\"\n",
    "        dJ_dw = [np.zeros(w.shape) for w in self.liste_w]\n",
    "\n",
    "        # propagation_avant\n",
    "        activation = np.vstack((np.ones(1),x)) # activation\n",
    "        activations = [np.vstack((np.ones(1),x))] # liste des activations couche par couche\n",
    "        zs = [] # liste des z par couche\n",
    "        for w in self.liste_w:\n",
    "            z = np.dot(w.transpose(),activation)\n",
    "            zs.append(z)\n",
    "            activation = np.vstack((np.ones(1),sigmoide(z))) \n",
    "            activations.append(activation)\n",
    "        \n",
    "        # retropropagation\n",
    "        dJ_dz = self.dJ_da_final(activations[-1][1:], y)*derivee_sigmoide(zs[-1])\n",
    "        dJ_dw[-1] = np.dot(activations[-2],dJ_dz.transpose())\n",
    "        # itérer de la couche nc-2 à la couche 1\n",
    "        for l in range(2, self.nombre_couches):\n",
    "            z = zs[-l]\n",
    "            sp = derivee_sigmoide(z)\n",
    "            dJ_dz = np.dot(self.liste_w[-l+1], dJ_dz)[1:] * sp\n",
    "            dJ_dw[-l] = np.dot(activations[-l-1], dJ_dz.transpose())\n",
    "        return dJ_dw\n",
    "\n",
    "    def metriques(self, donnees):\n",
    "        \"\"\"Métriques pour régression\"\"\"\n",
    "        erreur_quadratique = 0\n",
    "        erreur_absolu = 0\n",
    "        for (x,y) in donnees:\n",
    "            resultat_propagation = self.propagation_avant_w(np.vstack((np.ones(1),x)))[1:]\n",
    "            erreur_quadratique += sum((resultat_propagation-y)**2)\n",
    "            erreur_absolu += sum(abs(resultat_propagation-y))\n",
    "        return (erreur_quadratique,erreur_absolu)\n",
    "\n",
    "    def dJ_da_final(self, output_activations, y):\n",
    "        \"\"\"Dérivée de J par rapport à l'activation\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "def diviser_ent_test(donnees, proportion):\n",
    "    \"\"\" Diviser aléatoirement les données en deux groupes, entrainement et test\n",
    "    proportion : proportion des données de test\"\"\"\n",
    "    random.shuffle(donnees)\n",
    "    taille_test = int(len(donnees) * proportion)\n",
    "    return donnees[:taille_test],donnees[taille_test:]\n",
    "\n",
    "def bitmap2(classe):\n",
    "    \"\"\" Representer l'entier de classe par un vecteur bitmap (10,1) \n",
    "    classe : entier entre 0 et 9 qui représente la classe de l'observation\"\"\"\n",
    "    e = np.zeros((2, 1))\n",
    "    e[classe] = 1.0\n",
    "    return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Régression linéaire multivariée avec RNA\n",
    "# Données Boston Housing\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "maisons_boston = datasets.load_boston()\n",
    "maisons_X = maisons_boston.data\n",
    "maisons_y = maisons_boston.target\n",
    "    \n",
    "donnees=[(np.reshape(maisons_X[i], (2, 1)),np.reshape(maisons_y[i],(1,1))) for i in range(len(maisons_X))]\n",
    "donnees_test,donnees_ent=diviser_ent_test(donnees, 0.5)\n",
    "\n",
    "un_rna = RNA([13,5,1])\n",
    "un_rna.entrainer_par_mini_lot(donnees_ent,donnees_test,100,10,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Régression linéaire multivariée avec sklearn\n",
    "# Données Boston Housing\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "maisons_boston = datasets.load_boston()\n",
    "maisons_X = maisons_boston.data\n",
    "maisons_y = maisons_boston.target\n",
    "\n",
    "# Estimation des paramètres avec le modèle de régression linéaire\n",
    "modele = linear_model.LinearRegression()\n",
    "modele.fit(maisons_X, maisons_y)\n",
    "\n",
    "# Prédictions avec les observations\n",
    "maisons_y_estime = modele.predict(maisons_X)\n",
    "\n",
    "print(\"Theta0:\", modele.intercept_)\n",
    "print(\"Theta1-n:\", modele.coef_)\n",
    "print(\"Erreur quadratique moyenne :\", mean_squared_error(maisons_y_estime,maisons_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
